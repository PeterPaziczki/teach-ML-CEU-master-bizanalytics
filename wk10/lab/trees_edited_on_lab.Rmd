---
title: "Lab week 10 - Tree-based methods"
subtitle: "Data Science and Machine Learning 2 - CEU 2018"
author: "Jeno Pal"
date: '2018-03-06'
output:
  html_document:
    df_print: paged
  html_notebook:
    df_print: paged
---



```{r, message=FALSE}
library(data.table)
library(caret)
library(rpart)
library(rpart.plot)
library(xgboost)
library(randomForest)
library(gbm)
library(ISLR)
library(skimr)
```

## Bagging, random forests

We covered decision trees in DS-ML-1. Using it as a base model lets us
build many different models with less variance and better predictive power.
The downside: interpretation gets harder.

Idea: as individual trees are unstable and have high variance, train many
versions on bootstrap samples ("Bagging": Bootstrap AGGregation).
Then predict: take the average (regression),
majority vote / class share (classification). 

Random forests: randomly constrain the set of predictor variables used to
grow trees. Goal: avoid correlated trees that are very similar to each other,
still with the aim of decreasing variance.

```{r}
data(Hitters)
data <- data.table(Hitters)
skim(data)
```

```{r}
# goal: predict log salary
data <- data[!is.na(Salary)] # dropping obs without salary
data[, log_salary := log(Salary)]
data[, Salary := NULL]
```


```{r}
training_ratio <- 0.75 
set.seed(1234)
train_indices <- createDataPartition(y = data[["log_salary"]],
                                     times = 1,
                                     p = training_ratio,
                                     list = FALSE)
data_train <- data[train_indices, ]
data_test <- data[-train_indices, ]
```

Let's see benchmarks: a linear model and a simple regression tree.
```{r}
train_control <- trainControl(method = "repeatedcv",
                              number = 5,
                              repeats = 3)
# linear model with all the features to have a benchmark, 5-fold CV repeated 3 times

set.seed(857)
linear_model <- train(log_salary ~ .,
                      method = "lm",
                      data = data_train,
                      trControl = train_control)
linear_model
```

```{r}
set.seed(857)
simple_tree_model <- train(log_salary ~ .,
                      method = "rpart",
                      data = data_train,
                      tuneGrid = data.frame(cp = c(0.01, 0.02, 0.05)),
                      trControl = train_control)
# 0.02 complexity parameter is chosen
simple_tree_model
```

```{r}
rpart.plot(simple_tree_model[["finalModel"]])
# it summarizes the tree in a nice way
# CAtBat is chosen for the first cut
# the first one
```

For random forests,
`mtry` sets the number of variables randomly chosen for a tree. When `mtry`
equals the number of features, it is the bagging.

```{r}
# random forest
set.seed(857)
rf_model <- train(log_salary ~ .,
                  method = "rf",
                  data = data_train,
                  trControl = train_control,
                  tuneGrid = data.frame(mtry = c(2, 3, 5, 7, 9, 12, 19)),
                  importance = T # to calculate variable importance measures
                  )
# random forest has two parameters: number of trees to build (default is 500) and number of features to randomly use (mtry)
# in this example we have 19 features, so running random forest with all the features is bagging (boostrap aggregating)
rf_model
```

```{r}
# the number of trees is not a tuning parameter with caret
# default is 500, you can change it with passing the parameter to train
set.seed(857)
rf_model_ntree_10 <- train(log_salary ~ .,
                  method = "rf",
                  data = data_train,
                  trControl = train_control,
                  tuneGrid = data.frame(mtry = c(2, 3, 5)),
                  ntree = 10,
                  importance = T # to calculate variable importance measures
                  )
rf_model_ntree_10
```

```{r}
# calculate test error
RMSE <- function(x, true_x) sqrt(mean((x - true_x)^2))

RMSE(data_test[["log_salary"]], predict.train(rf_model, newdata = data_test))
```

(It is a small dataset hence performance measures can have large
variances.)

### Variable importance

With the ensemble models we have a hard time with interpretation.
Variable importance measures can help to see which features contribute most
to the predictive power of models. The generic `varImp` function of `caret`
does model-specific calculations, consult [here](https://topepo.github.io/caret/variable-importance.html) for a description
for your model at hand.

```{r}
varImp(rf_model)
# we have 500 trees, so how to determine the variable importance in case of 500 trees?
# We take the average of how much that variable contributed to lowering RMSE in each tree
```

```{r}
plot(varImp(rf_model))
# rescaling everything to the best, it is CHits in this case
# when using random forest, we can have different variable importance plots for each tree
```

## Gradient boosting machines

Gradient boosting machines: also ensembles of trees, however,
the method of choosing them is different. Idea: get the residual and train
next tree to predict (explain) the residual. Then add it to the previous
trees, with a shrinkage parameter (to avoid overfitting).

Another difference: GBMs use shallow trees (controlled by
`interaction.depth`) whereas RFs use unpruned, large trees (with low bias
and high variance). Common part: idea of bagging.

```{r}
# we are also building many trees here, that is a similarity

# difference between random forest and gradient boosting: gradiant boost does not necessarily need to be a tree, it can be something else. the way of combining trees is different. We look at residuals, then we try to explain that.
# It works sequentally, meaning, that first we build a tree, compute the residuals, then build a model on residuals too. Then I build a new model, compute the residuals and model that too, and so on
# we use shrinkage here as well. lambda is the shrinkage parameter, serves the purpose of regularization, it determines how much I am gonne take the model fitted on residuals into account.

# trees do not play a crucial part here, it could be anything else.

gbm_grid <- expand.grid(n.trees = c(100, 500, 1000), # deciding how many trees i want to build, is my decision
                        interaction.depth = c(2, 3, 5), # how large trees I want to use, the larger, the more complex individual trees I have
                        shrinkage = c(0.005, 0.01, 0.1), # the smaller the shrinkage, the less I take into account the model fitted to residuals
                        n.minobsinnode = c(5)) # a threshold for obs in a node
# we have 27 parameter combinations here, not to mentione doing 5-fold CV 3 times
# it can be computation heavy
  
gbm_model <- train(log_salary ~ .,
                   method = "gbm",
                   data = data_train,
                   trControl = train_control,
                   tuneGrid = gbm_grid,
                   verbose = FALSE # gbm by default prints too much output
                   )
gbm_model
```

4 hyperparameters: [the curse of dimensionality](https://en.wikipedia.org/wiki/Curse_of_dimensionality) 
can make it hard to come up
with efficient grids for hyperparameter search. However, with `gbm` and many
other models, in reality the models to be estimated are not exponentially
growing with the number of hyperparameters (see [here](https://topepo.github.io/caret/random-hyperparameter-search.html)).

```{r}
plot(varImp(gbm_model))

# CAtBat is dominant, but wasn't in the random forest case. the reason for it is that in case of random forest we did not let this variable to contribute to all the modesl, because we randomly selected what variables can be used for splits. But here this variable can contribute to all the models
```

Lower `eta` means slower learning, hence more trees are necessary to have
good performance.

Not tuned: `bag.fraction` parameter (set to default 0.5): for the construction
of each tree, only `bag.fraction` share of the sample is used (randomly
selected, see `?gbm`). 
This, again, is the same idea as with bagging: decrease
variance of the model. You can pass another value for it via
giving `train` an argument `bag.fraction`, just like we saw with
`rf` and `ntree`.

```{r}
gbm_grid_2 <- data.frame(n.trees = c(1000), 
                         interaction.depth = c(5), 
                         shrinkage = c(0.005),
                         n.minobsinnode = c(5))
  
gbm_model_2 <- train(log_salary ~ .,
                   method = "gbm",
                   data = data_train,
                   trControl = train_control,
                   tuneGrid = gbm_grid_2,
                   bag.fraction = 0.8,
                   verbose = FALSE # gbm by default prints too much output
                   )
gbm_model_2
```


### XGBoost

A celebrated implementation of the gradient boosting idea. 
_"Both xgboost and gbm follows the principle of gradient boosting. There are however, the difference in modeling details. Specifically, xgboost used a more regularized model formalization to control over-fitting, which gives it better performance."_


See documentation [here](http://xgboost.readthedocs.io/).
It proved to be very stable and widely applicable. For the many hyperparameters,
consult the documentation.
```{r}
# it is a gradient boosting algo - uses more tricks and regularization
# they mastered the original gradient boosting algo, superfinetuned it :)
# it can be faster, also implemented in python and h2o, can be accessed via caret
# tuning parameters have different name
# robust - popular in kaggle competitions

xgbGrid <- expand.grid(nrounds = c(500, 1000), # number of trees = n.trees
                       max_depth = c(2, 3, 5), # interaction.depth
                       eta = c(0.01, 0.05), # shrinkage parameter
                       gamma = 0, # complexity parameter
                       colsample_bytree = c(0.5, 0.7),
                       min_child_weight = 1, # number of obs in final nodes
                       subsample = c(0.5)) # similar to bag parameter to GBM, 
set.seed(857)
xgboost_model <- train(log_salary ~ .,
                       method = "xgbTree",
                       data = data_train,
                       trControl = train_control,
                       tuneGrid = xgbGrid)
xgboost_model
```

```{r}
plot(varImp(xgboost_model))
```


```{r}
resamples_object <- resamples(list("rpart" = simple_tree_model,
                                   "rf" = rf_model,
                                   "gbm" = gbm_model,
                                   "xgboost" = xgboost_model))
summary(resamples_object)
```




