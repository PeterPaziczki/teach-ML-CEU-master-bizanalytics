---
title: "Lab week 4 - Unsupervised methods"
subtitle: "Data Science and Machine Learning 1 - CEU 2018"
author: "Jeno Pal"
date: '2018-01-30'
output:
  html_document:
    df_print: paged
  html_notebook:
    df_print: paged
---

```{r, message=FALSE}
library(data.table)
library(caret)
library(skimr)
library(datasets)
library(ggplot2)

library(ISLR)
library(NbClust)
library(factoextra)

theme_set(theme_minimal())
```

## Principal Component Analysis (PCA)

We transform the coordinates of the original variables to capture as much
variation as we can with independent (orthogonal) dimensions.

```{r}
data <- USArrests
# data <- data.table(data, keep.rownames = TRUE)
# setnames(data, "rn", "state")
print(skim(data))
```

Scaling: necessary to have comparable units of different variables.
(Multiplying a variable by 100 increases the variance and PCA would try to
capture that, even though it is only an artifact of units. We don't want that.)
```{r}
pca_result <- prcomp(data, scale. = TRUE) # when calling prcomp defining scale is a must, because the default is not to sclae. prcomp is base R command.
## prcomp does the centering by default (mean is zero and standard deviation is one).
print(pca_result) ## each coolumn should have a sum of squared of 1
## the PCs are orthogonal to each other
```

```{r}
names(pca_result)
```

```{r}
# weights sum to one
colSums(pca_result$rotation^2)
# PCA components are orthogonal
sum(pca_result$rotation[, 1] * pca_result$rotation[, 2])
```

We can plot observations as well as original features in the space spanned
by the first two principal components.
```{r}
fviz_pca(pca_result, scale = 0)
## being negative does not mean anything here
```

We can plot how much of total variance is captured by subsequent principal
components (and in total). Total variance: if scaling was done, it equals
the number of variables (since it is 1 for each variable).
```{r}
variances <- pca_result$sdev^2
total_variance <- sum(variances)
total_variance
variances
## the sum of total variance is four, because we have four variables
```

```{r}
share_variance_by_component <- variances / total_variance
dt_variance <- data.table(component = 1:length(variances),
                          share_variance = share_variance_by_component)
dt_variance[, cum_share_variance := cumsum(share_variance)]
```

```{r}
ggplot(data = melt(dt_variance, id.vars = "component")) +
  geom_line(aes(x = component, y = value, color = variable)) +
  facet_wrap(~ variable, scales = "free_y") +
  theme(legend.position = "bottom")
## what share of the variance is captured by principle component
## the first two PCs capture the 90% of total variance
## there is no randomness here, it always have one solution, it is determenistic.
```

How many components summarize the data? No definite answer: decide based
on sufficient variance explained.

### PCA with `caret`

We can use `preProcess` from `caret` to perform the same transformations. These
can serve as inputs to `train`.
```{r}
pre_process <- preProcess(data, method = c("center", "scale", "pca")) # i can tell how to preprocess datat (it is part of caret)
pre_process # whenever i ask caret to PCA, by default it will filter the data to have 95% of the variance. It can be configured of course
```
```{r}
pre_process$rotation ## PC4 has been dropped
```

```{r}
pre_process <- preProcess(data, method = c("center", "scale", "pca"), pcaComp = 4) # with pcaComp i can set how many PCs I want to preserve
pre_process$rotation
```

```{r}
preProcess(data, method = c("center", "scale", "pca"), thresh = 0.999) # i can set the tershold, to what extent I want to capture variance
```

### Using PCA as an input to supervised learning

Let's predict baseball player salaries using PCA.
```{r}
data <- ISLR::Hitters
data <- data.table(data)
print(skim(data))
```
```{r}
data <- data[!is.na(Salary)]
```

Train a simple linear model:
```{r}
set.seed(857)
lm_fit <- train(Salary ~ . , 
                data = data, 
                method = "lm", 
                trControl = trainControl(method = "cv", number = 10),
                preProcess = c("center", "scale")
                )
lm_fit # data was not divided into parts, just wanted to use CV t compare models. If we use the same seed, we can compare models.
```

We can use PCA with specified number of components (or we can also
set the `thresh` argument to set a threshold that PCA components
explain at least a certain share of the variance):
```{r}
set.seed(857)
lm_fit_pca <- train(Salary ~ . , 
                    data = data, 
                    method = "lm", 
                    trControl = trainControl(
                      method = "cv", 
                      number = 10,
                      preProcOptions = list(pcaComp = 18)), # we have 19 features, so we reduce the dimension with one
                    preProcess = c("center", "scale", "pca")
)
lm_fit_pca # it is going to perform a pca and use those variables to build a model and predict
# first everything is centered and scaled, then it does pca. The previous and this result is totally comparable, because having pca is only an internal step
# pca helps filter the noise.
# we can see that with having one less component we have a better RMSE.
# in this case we have a few obs but many predictors.
```

Method `pcr` implements precisely this: linear regression with principal
components as explanatory variables. Its hyperparameter is the number of
components to be used.
```{r}
tune_grid <- data.frame(ncomp = 1:19) # it performs it with n number of PCs
set.seed(857)
pcr_fit <- train(Salary ~ . , 
                data = data, 
                method = "pcr", # it chooses the number of PCs itself. It is a linear regression with PCs as explonatory variables.
                trControl = trainControl(method = "cv", number = 10),
                tuneGrid = tune_grid,
                preProcess = c("center", "scale")
                )
pcr_fit
```

## K-means clustering

Goal: group observations that are more similar to each other than to others.

```{r}
data <- data.table(iris)
setnames(data, c("sepal_length",
                 "sepal_width", 
                 "petal_length",
                 "petal_width", 
                 "species"))
print(skim(data)) # it is an iterative algo, it converges to one final solution, which depends on the starting points
```

```{r}
ggplot(data, aes(x = petal_length, y = sepal_length, color = species)) + geom_point()
```

Suppose we do not know labels but want to group observations
based on features.
```{r}
data_features <- data[, .(petal_length, petal_width, sepal_length, sepal_width)] # just getting rid of labels
km <- kmeans(data_features, centers = 3)
km # it gives back the final cluster centers
```

```{r}
data_w_clusters <- cbind(data_features, 
                         data.table("cluster" = factor(km$cluster)))

ggplot(data_w_clusters, 
       aes(x = petal_length, y = sepal_length, color = cluster)) + 
  geom_point()
```

We can inspect the resulting centers.
```{r}
km$centers
```

```{r}
centers <- data.table(km$centers)
str(centers)
centers[, cluster := factor("center", levels = c(1, 2, 3, "center"))]

data_w_clusters_centers <- rbind(data_w_clusters, centers)
ggplot(data_w_clusters_centers, 
       aes(x = petal_length, 
           y = sepal_length, 
           color = cluster,
           size = ifelse(cluster == "center", 2, 1))) + 
  geom_point() +
  scale_size(guide = 'none')
```

Results depend on the starting centers which are randomly chosen
observations.

```{r}
set.seed(1122)
km <- kmeans(data_features, centers = 3, nstart = 1) # nstart - random selection of initial points is done n times. It shouldn't be one, but 20 for example
print(km$centers)
print(table(km$cluster))
print(km$withinss)

set.seed(123)
km <- kmeans(data_features, centers = 3, nstart = 1)
print(km$centers)
print(table(km$cluster))
print(km$withinss)
```

We should always experiment with
different starting values (probably generated randomly).
`nstart` controls how many times the algorithm is run with different
random starting ponits for the centers. Setting it to a high value
(e.g., 20) is a good idea to achieve the best groupings.

### Choosing K

There are no general rules, depends on the application. There are some
rules of thumb, though. There are many, you can explore some
with explanation, [here](http://www.sthda.com/english/wiki/print.php?id=239).
`NbClust` calculates 30 indices based on various principles and chooses by
majority rule.

```{r, results="hide"}
nb <- NbClust(data_features, method = "kmeans", 
              min.nc = 2, max.nc = 10, index = "all") # the output to console is the most interesting
```
```{r}
fviz_nbclust(nb)
```

```{r}
# the gap method illustrated in the lecture
nb <- NbClust(data_features, method = "kmeans", 
              min.nc = 2, max.nc = 10, index = "gap")
```

## Hierarchical clustering

With hierarchical clustering we get a nested structure of clusters
based on a dissimilarity measure. Is it better than k-means? It depends - 
k-means does not yield a hierarchical structure. If the data does not have
one in reality, hierarchical may be not as good as k-means and there are
reversed situations as well.

```{r}
data_distances <- dist(data_features)
# use the average distance between groups to decide which
# groups to merge next
hc <- hclust(data_distances, method = "average")
```

```{r}
fviz_dend(hc)
```

```{r}
fviz_dend(hc, k = 3)
```

```{r}
fviz_dend(hc, k = 6)
```

```{r}
# get labels
cluster_hc <- cutree(hc, 3)
data_w_clusters <- cbind(data_features, data.table("cluster" = factor(cluster_hc)))

ggplot(data_w_clusters, 
       aes(x = petal_length, y = sepal_length, color = cluster)) + 
  geom_point()
```

How we calculate similarities between groups may strongly affect the clustering.
```{r}
# max distance between points of two groups
hc_complete <- hclust(data_distances, method = "complete")
fviz_dend(hc_complete, k = 3)
```

```{r}
# single: minimal distance between points of two groups
hc_single <- hclust(data_distances, method = "single")
fviz_dend(hc_single, k = 3)
```

## General considerations on clustering

- as both clustering methods are based on distances in features, we may
want to first bring all variables to the same scale
- clusters may be sensitive to details such as scaling or not, whether we take subsets of data or what is the dissimilarity measure in hierarchical clustering. Advice: experiment with the settings and look for consistent patterns.