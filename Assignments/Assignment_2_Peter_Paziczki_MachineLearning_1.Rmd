---
title: "Homework assignment 2"
subtitle: "Data Science and Machine Learning 1 - CEU 2018"
author: "Peter Paziczki"
date: '2018 február 5'
output: html_document
---

<style>
body {
text-align: justify}
</style>

```{r setup, include=FALSE}
rm(list=ls())
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(data.table)
library(caret)
library(bit64)
library(ggplot2)
library(GGally)
library(gplots)
library(ROCR) # performance function
#library(e1071)
#library(glmnet)
```

# 1. Predicting mental health problems in the tech sector

The variable to predict is treatment.

```{r}
data <- fread("../data/mental-health-in-tech/survey_cleaned.csv")

data <- data[ ,c("comments", "state","work_interfere") := NULL]
data[, age := as.numeric(age)]
data[ , treatment := factor(treatment, levels = c("Yes", "No"))]

## some additional data cleaning
data <- data[age %between% c(18, 100)]

## making treatment variable into a binary variable
data[, treatment_binary := ifelse(treatment == "Yes", 1, 0)]
data <- data[ ,c("Timestamp") := NULL]

## creating large_countries vector to filter small countries out of the dataset
data[,.N, by = country]
large_countries <- data[, .N, by = "country"][N > 10][["country"]]
data <- data[country %in% large_countries]
```

## 1.1 Explore some predictors that can be used to predict treatment.

I have transformed the `treatment` variable into a binary variable, denoting 1 to yes. The below chart shows that there is definitely some relation between age and the ratio of people seeking treatment for a mental health condition. It is important to mentione that there is only three observation in the 60-65 range and only one in 70-75 age range.

```{r}
data_by_age <- data[ ,
  .(treatment_rate = mean(treatment_binary), num_obs = .N),
  keyby = .(age_category = cut(age, breaks = seq(0, 100, by = 5), include.lowest = TRUE))]

ggplot(data = data_by_age, aes(x = age_category, y = treatment_rate, size = num_obs)) +
  geom_point() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))
```

The below chart assesses the treatment ratio by gender. The survey has been mostly filled by male employees who, on average, tend to least seek treatment compared to female and trans gender cooworkers. 

```{r}
data[, .N, by = gender]

data_by_gender <- data[ ,
  .(treatment_rate = mean(treatment_binary), num_obs = .N),
  keyby = gender]

ggplot(data = data_by_gender, aes(x = gender, y = treatment_rate, size = num_obs)) +
  geom_point() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))
```

I found it to be interesting to assess how the treatment ratio varies across countries. Having all the countries does not make sense, there are several countries with 1 or a bit more observations.

```{r}
data[,.N, by = country]

data_by_country <- data[ ,
  .(treatment_rate = mean(treatment_binary), num_obs = .N),
  keyby = country]

ggplot(data = data_by_country, aes(x = country, y = treatment_rate, size = num_obs)) +
  geom_point() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))
```

I have dropped the countries that had less than `r 10` observations. Now with still having the 91.5% of the observations, the countries left in the data set could be good predictors. In France and India only a low portion of employees sought treatment, while in Australia it is the highest.

```{r}
large_countries <- data_by_country[num_obs >= 10]

ggplot(data = large_countries, aes(x = country, y = treatment_rate, size = num_obs)) +
  geom_point() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))
```

Those who have a family history of mental illness tend to seek treatment in a much higher ratio than those who don't have. I am expecting it to be a good predictor.

```{r}
data_family_history <- data[ ,
  .(treatment_rate = mean(treatment_binary), num_obs = .N),
  keyby = family_history]

ggplot(data = data_family_history, aes(x = family_history, y = treatment_rate, size = num_obs)) +
  geom_point() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))
```

Those whose employer provides mental health benefits tend to seek treatment in a higher ratio, it also seems to be a good predictor.

```{r}
data_benefits <- data[ ,
  .(treatment_rate = mean(treatment_binary), num_obs = .N),
  keyby = benefits]

ggplot(data = data_benefits, aes(x = benefits, y = treatment_rate, size = num_obs)) +
  geom_point() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))
```

It is quite interesting, but based on the data more difficult is to take advantage of a mental health or substance abuse treatment resources, the more employees tend to seek treatment.

```{r}
data_leave <- data[ ,
  .(treatment_rate = mean(treatment_binary), num_obs = .N),
  keyby = leave]

ggplot(data = data_leave, aes(x = leave, y = treatment_rate, size = num_obs)) +
  geom_point() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))
```

Please find a few examples below where I was expecting to find good predictors but they were found to be not good predictors.

Being self employed does not seem to be a good predictor, there is only a slight difference between the groups in treatment ratio.

```{r}
data_self_employed <- data[ ,
  .(treatment_rate = mean(treatment_binary), num_obs = .N),
  keyby = self_employed]

ggplot(data = data_self_employed, aes(x = self_employed, y = treatment_rate, size = num_obs)) +
  geom_point() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))
```

There is no significant difference in treatment ratio among different company sizes, considering the company size does not seem to be a good predictor.

```{r}
data_no_employee <- data[ ,
  .(treatment_rate = mean(treatment_binary), num_obs = .N),
  keyby = no_employees]

ggplot(data = data_no_employee, aes(x = no_employees, y = treatment_rate, size = num_obs)) +
  geom_point() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))
```

Having the anonymity protected does not seem to play a significant role in seeking or not seeking treatment.

```{r}
data_anonymity <- data[ ,
  .(treatment_rate = mean(treatment_binary), num_obs = .N),
  keyby = anonymity]

ggplot(data = data_anonymity, aes(x = anonymity, y = treatment_rate, size = num_obs)) +
  geom_point() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))
```

## 1.2 Partition your data to 70% training and 30% test samples.

```{r}
set.seed(1234)
training_ratio <- 0.7
train_indices <- createDataPartition(y = data[["treatment"]],
                                     times = 1,
                                     p = training_ratio,
                                     list = FALSE)
data_train <- data[train_indices, ]
data_test <- data[-train_indices, ]
```

## 1.3 Build models with `glmnet` and `rpart` that predict the binary outcome of `treatment` (you don’t have to use all variables if you don’t want to - experiment! Just use the same variables for both model families). Use cross-validation on the training set and use AUC as a selection measure (use `metric = "ROC"` in train and also don’t forget to use `classProbs = TRUE, summaryFunction = twoClassSummary` in  trainControl). Make sure to set the same seed before each call to train.

```{r}
# glmnet model for cross-validation, hold-out set AUCs
train_control <- trainControl(method = "cv",
                              number = 10,
                              classProbs = TRUE,
                              #verboseIter = TRUE,
                              summaryFunction = twoClassSummary)

tune_grid <- expand.grid("alpha" = seq(0.1 , 1, 0.1),
                         "lambda" = c(0.1, 0.01, 0.001, 0.0001))

set.seed(1234)
glmnet_model <- train(treatment ~
                      poly(age, 2) + gender + country + family_history + benefits + leave,
                      data = data_train,
                      method = "glmnet",
                      preProcess = c("center", "scale"),
                      trControl = train_control,
                      tuneGrid = tune_grid,
                      metric = "ROC")
glmnet_model
```



```{r}
tune_grid <- data.frame("cp" = c(0.1, 0.01, 0.001, 0.0001, 0.00001, 0.000001))

set.seed(1234)
rpart_model <- train(treatment ~ 
                      poly(age, 2) + gender + country + family_history + benefits + leave,
                      data = data_train,
                      method = "rpart",
                      preProcess = c("center", "scale"),
                      trControl = train_control,
                      tuneGrid = tune_grid,
                      metric = "ROC")
rpart_model
```

## 1.4 Compare models based on their predictive performance based on the cross-validation information (you can just use the mean AUC to select the best model).

`glmnet` provided a better performance, a higher ROC value: `0.7448523`, I am picking this model.

## 1.5 Evaluate the best model on the test set: draw an ROC curve and calculate and interpret the AUC.

Evaluating the glmnet model on test set:

```{r}
## evaluating the glmnet model on test set
test_prediction_glmnet_probs <- predict.train(glmnet_model, 
                                        newdata = data_test, 
                                        type = "prob") # obtaining probabilities instead of binary predictions
```

Drawing the ROC curve by using the built-in plot method:

```{r}
glmnet_prediction <- prediction(test_prediction_glmnet_probs$Yes,
                              data_test[["treatment"]])

## using the built-in plot method
plot(performance(glmnet_prediction, "tpr", "fpr"), colorize=TRUE) 
```

The ROC curve is a two-dimensional plot with false positive rate on the x axis and true positive rate on the y axis. The ROC curve is used to display the previously mentioned values. It can interpreted as displaying the true positive rates for any false positive rates. The higher the true positive rate is, the better the model is.

Drawing ROC curve by using ggplot:

```{r}
# a ggplot version
# using prediction function from ROCR package
glm_perf <- performance(glmnet_prediction, measure = "tpr", x.measure = "fpr")

glm_roc_df <- data.table(
  model = "glm",
  FPR = glm_perf@x.values[[1]],
  TPR = glm_perf@y.values[[1]],
  cutoff = glm_perf@alpha.values[[1]] # cutoff is the treshold
)

ggplot(glm_roc_df) +
  geom_line(aes(FPR, TPR, color = cutoff), size = 2) +
  geom_ribbon(aes(FPR, ymin = 0, ymax = TPR), fill = "blue", alpha = 0.1) +
  geom_abline(intercept = 0, slope = 1,  linetype = "dotted", col = "black") +
  scale_y_continuous(limits = c(0, 1), breaks = seq(0, 1, .1)) +
  scale_x_continuous(limits = c(0, 1), breaks = seq(0, 1, .1)) +
  xlab("False Positive Rate") + ylab("True Positive Rate") 
```

The dotted line has a slope of 1 and serves as a visual aid, the area under it is 0.5. If classification had been done randomly with true probabilitay, it would have the curve of that.

Calculating AUC:

```{r}
## calculating AUC = area under the curve -> the measure of predictive power
AUC <- performance(glmnet_prediction, "auc")@y.values[[1]]
print(AUC)
```

AUC stands for *Area under the curve*, it is practically the integral of ROC curve, the higher the value, the better the prediction is. It is a model independent way to assess the predictive performance of the model.

## 1.6 If you have to choose a probability threshold to predict the outcome, what would you choose? At this threshold, how large are the true positive rate and the false positive rate? How many false positives and false negatives there are in the test sample?

Calling a `confusionMatrix` to see how many true and false values the model predicts:

```{r}
test_truth <- data_test[["treatment"]]
test_prediction_glmnet <- predict.train(glmnet_model, 
                                        newdata = data_test) # obtaining binary prediction instead of probabilities

confusionMatrix(test_prediction_glmnet, test_truth)
```

The accuracy shows the share of rightly predicted outcomes with the default threshold of `predict.train` function. It uses a 50% threshold for prediction by default, meaning that if prediction is more than or equal to 0.5, it will be labelled as yes and no otherwise. Running the model with the different thresholds:

```{r}
thresholds <- seq(0.1, 0.95, by = 0.05)

true_positive_rates <- rep(0, length(thresholds)) 
false_positive_rates <- rep(0, length(thresholds)) 

for (ix in 1:length(thresholds)) {
  thr <- thresholds[ix]
  test_prediction <- ifelse(test_prediction_glmnet_probs$Yes > thr, "Yes", "No")
  test_prediction <- factor(test_prediction, levels = c("Yes", "No"))
  cm <- as.matrix(confusionMatrix(test_prediction, test_truth))
  true_positive_rates[ix] <- cm[1, 1] / (cm[1, 1] + cm[2, 1])
  false_positive_rates[ix] <- cm[1, 2] / (cm[1, 2] + cm[2, 2])
  print(paste("Threshold:", thr))
  print(confusionMatrix(test_prediction, test_truth)[["table"]])
} 
```

After scannig through a few possible thresholds, 0.4 seems to be the one that gives the most true predictions. The true positive rate is the ratio of true positive predictions and the sum of all positive outcomes, it is `r true_positive_rates[7]`. The false negative rate is the share of false positives in all the negative outcomes, it is `r false_positive_rates[7]`. When having a threshold of 0.4 we have 80 false positives and 31 false negatives.

```{r}
true_positive_rates[7]
false_positive_rates[7]
```

Please find the true positive and false positive rates for all the thresholds displayed on the following plot:

```{r}
manual_roc <- data.table("threshold" = thresholds,
                         "true_positive_rate" = true_positive_rates,
                         "false_positive_rate" = false_positive_rates)
ggplot(data = manual_roc, 
       aes(x = false_positive_rate,
           y = true_positive_rate,
           color = threshold)) +
  geom_point()
```

# 2. Transformed scores

Take the medical appointment no-show dataset we used in class and apply all the cleaning steps we did, then create a training and a test set. Estimate a predictive model of your choice for no_show as a target variable. Get predicted scores (probabilities). Then calculate two transformations of the scores: take the square root and the square of the probabilities. These are valid scores as well, they are alse between 0 and 1 so they can be used for classification.

```{r}
data <- fread("../data/medical-appointments-no-show/no-show-data.csv")

# [... apply the cleaning steps we did in class ...]

# some data cleaning
data[, c("PatientId", "AppointmentID", "Neighbourhood") := NULL]
setnames(data, 
         c("No-show", 
           "Age", 
           "Gender",
           "ScheduledDay", 
           "AppointmentDay",
           "Scholarship",
           "Hipertension",
           "Diabetes",
           "Alcoholism",
           "Handcap",
           "SMS_received"), 
         c("no_show", 
           "age", 
           "gender", 
           "scheduled_day", 
           "appointment_day",
           "scholarship",
           "hypertension",
           "diabetes",
           "alcoholism",
           "handicap",
           "sms_received"))
# clean up a little bit
data <- data[age %between% c(0, 95)]
# for binary prediction with caret, the target variable must be a factor
data[, no_show := factor(no_show, levels = c("Yes", "No"))] # making it a factor is necessary because of using caret
data[, no_show_num := ifelse(no_show == "Yes", 1, 0)]
data[, handicap := ifelse(handicap > 0, 1, 0)]

# create new variables
data[, scheduled_day := as.Date(scheduled_day)]
data[, appointment_day := as.Date(appointment_day)]
data[, days_since_scheduled := as.integer(appointment_day - scheduled_day)]
data <- data[days_since_scheduled > -1]
```

```{r}
# [... create train and test sets ... ]

# set up training and test data
# model selection will be done via cross-validation

training_ratio <- 0.5 
set.seed(1234)
train_indices <- createDataPartition(y = data[["no_show"]],
                                     times = 1,
                                     p = training_ratio,
                                     list = FALSE)
data_train <- data[train_indices, ]
data_test <- data[-train_indices, ]
```

```{r}
# glmnet model for cross-validation, hold-out set AUCs
train_control <- trainControl(method = "cv",
                              number = 10,
                              classProbs = TRUE,
                              #verboseIter = TRUE,
                              summaryFunction = twoClassSummary)

tune_grid <- expand.grid("alpha" = seq(0.1 , 1, 0.1),
                         "lambda" = c(0.1, 0.01, 0.001, 0.0001))

set.seed(1234)
glmnet_model <- train(no_show ~
                      poly(age, 2) + gender + days_since_scheduled,
                      data = data_train,
                      method = "glmnet",
                      preProcess = c("center", "scale"),
                      trControl = train_control,
                      tuneGrid = tune_grid,
                      metric = "ROC")
glmnet_model
```

```{r}
prediction <- predict.train(glmnet_model, newdata = data_test, type = "prob")
prediction_sqrt <- sqrt(prediction)
prediction_sq <- data.frame(prediction^2)
```

## 2.1 Draw ROC curves for all three scores and calculate the AUC. How do they compare? Is it surprising in light of the interpretation of the AUC?

```{r}
# a ggplot version
# using prediction function from ROCR package
glmnet_prediction <- prediction(prediction$Yes,
                              data_test[["no_show"]])
glmnet_prediction_sqrt <- prediction(prediction_sqrt$Yes,
                              data_test[["no_show"]])
glmnet_prediction_sq <- prediction(prediction_sq$Yes,
                              data_test[["no_show"]])

glm_perf <- performance(glmnet_prediction, measure = "tpr", x.measure = "fpr")

glmnet_roc_prediction_df <- data.table(
  model = "glmnet_predicion",
  FPR = glm_perf@x.values[[1]],
  TPR = glm_perf@y.values[[1]],
  cutoff = glm_perf@alpha.values[[1]] # cutoff is the treshold
)

glm_perf_sqrt <- performance(glmnet_prediction_sqrt, measure = "tpr", x.measure = "fpr")

glmnet_roc_prediction__sqrt_df <- data.table(
  model = "glmnet_prediction_sqrt",
  FPR = glm_perf_sqrt@x.values[[1]],
  TPR = glm_perf_sqrt@y.values[[1]],
  cutoff = glm_perf_sqrt@alpha.values[[1]] # cutoff is the treshold
)

glm_perf_sq <- performance(glmnet_prediction_sq, measure = "tpr", x.measure = "fpr")

glmnet_roc_prediction__sq_df <- data.table(
  model = "glmnet_prediction_sq",
  FPR = glm_perf_sq@x.values[[1]],
  TPR = glm_perf_sqrt@y.values[[1]],
  cutoff = glm_perf_sq@alpha.values[[1]] # cutoff is the treshold
)

roc_df <- rbind(glmnet_roc_prediction_df, glmnet_roc_prediction__sqrt_df, glmnet_roc_prediction__sq_df)

ggplot(roc_df) +
  geom_line(aes(FPR, TPR, color = model), size = 2) +
  geom_abline(intercept = 0, slope = 1,  linetype = "dotted", col = "black") +
  scale_y_continuous(limits = c(0, 1), breaks = seq(0, 1, .1)) +
  scale_x_continuous(limits = c(0, 1), breaks = seq(0, 1, .1)) +
  xlab("False Positive Rate") + ylab("True Positive Rate") 
```

```{r}
glmnet_prediction <- prediction(prediction$Yes,
                              data_test[["no_show"]])

## using the built-in plot method
plot(performance(glmnet_prediction, "tpr", "fpr"), colorize = TRUE) 
```

```{r}
## calculating AUC = area under the curve -> the measure of predictive power
AUC <- performance(glmnet_prediction, "auc")@y.values[[1]]
print(AUC)
```

```{r}
glmnet_prediction_sqrt <- prediction(prediction_sqrt$Yes,
                              data_test[["no_show"]])

## using the built-in plot method
plot(performance(glmnet_prediction_sqrt, "tpr", "fpr"), colorize=TRUE) 
```

```{r}
## calculating AUC = area under the curve -> the measure of predictive power
AUC <- performance(glmnet_prediction_sqrt, "auc")@y.values[[1]]
print(AUC)
```

```{r}
glmnet_prediction_sq <- prediction(prediction_sq$Yes,
                              data_test[["no_show"]])

## using the built-in plot method
plot(performance(glmnet_prediction_sq, "tpr", "fpr"), colorize=TRUE) 
```

```{r}
## calculating AUC = area under the curve -> the measure of predictive power
AUC <- performance(glmnet_prediction_sq, "auc")@y.values[[1]]
print(AUC)
```

All three have the same ROC curve and the same AUC value. As they have the same ROC curve, it is not suprising that the integrals of the curves are the same. The question rather is, why they have the same ROC curve. 

## 2.2 What is the key, common property of both the square root and the square functions that leads to this finding?

The ROC curve gives a true positive rate for each false positive rates for different cutoff points. Having the square root or square of *prediction* won't change the curve as the ratio of TPR and FPR won't change and they are to fit in a 1 by 1 scatterplot, they would produce the same curve.

## 2.3 Draw a calibration plot for all three scores separately:
+group people into bins based on predicted scores
+display on a scatterplot the mean of the predicted scores versus the actual share of people surviving

```{r}
truth_numeric <- ifelse(test_truth == "Yes", 1, 0)
score_glmnet <- prediction$Yes

summary(score_glmnet)
```

```{r}
actual_vs_predicted <- data.table(actual = truth_numeric,
                                  predicted = score_glmnet)

actual_vs_predicted[, score_category := cut(predicted,
                                    seq(0, 0.6, 0.05),
                                    include.lowest = TRUE)]
calibration <- actual_vs_predicted[, .(mean_actual = mean(actual),
                                       mean_predicted = mean(predicted),
                                       num_obs = .N),
                                   keyby = .(score_category)]
ggplot(calibration,
       aes(x = mean_actual, y = mean_predicted, size = num_obs)) +
  geom_point() +
  geom_abline(intercept = 0, slope = 1, linetype = "dashed") +
  ylim(0, 1) + xlim(0, 1)
```

```{r}
truth_numeric <- ifelse(test_truth == "Yes", 1, 0)
score_glmnet <- prediction_sqrt$Yes

summary(score_glmnet)
```

```{r}
actual_vs_predicted <- data.table(actual = truth_numeric,
                                  predicted = score_glmnet)

actual_vs_predicted[, score_category := cut(predicted,
                                    seq(0, 0.6, 0.05),
                                    include.lowest = TRUE)]
calibration <- actual_vs_predicted[, .(mean_actual = mean(actual),
                                       mean_predicted = mean(predicted),
                                       num_obs = .N),
                                   keyby = .(score_category)]
ggplot(calibration,
       aes(x = mean_actual, y = mean_predicted, size = num_obs)) +
  geom_point() +
  geom_abline(intercept = 0, slope = 1, linetype = "dashed") +
  ylim(0, 1) + xlim(0, 1)
```

```{r}
truth_numeric <- ifelse(test_truth == "Yes", 1, 0)
score_glmnet <- prediction_sq$Yes

summary(score_glmnet)
```

```{r}
actual_vs_predicted <- data.table(actual = truth_numeric,
                                  predicted = score_glmnet)

actual_vs_predicted[, score_category := cut(predicted,
                                    seq(0, 0.6, 0.05),
                                    include.lowest = TRUE)]
calibration <- actual_vs_predicted[, .(mean_actual = mean(actual),
                                       mean_predicted = mean(predicted),
                                       num_obs = .N),
                                   keyby = .(score_category)]
ggplot(calibration,
       aes(x = mean_actual, y = mean_predicted, size = num_obs)) +
  geom_point() +
  geom_abline(intercept = 0, slope = 1, linetype = "dashed") +
  ylim(0, 1) + xlim(0, 1)
```

+How do they compare? Which score(s) can be regarded as well-calibrated probabilites?

???