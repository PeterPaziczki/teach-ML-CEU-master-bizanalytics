---
title: "Homework assignment 3"
subtitle: "Data Science and Machine Learning 1 - CEU 2018"
author: "Peter Paziczki"
date: '2018 febru√°r 11'
output: html_document
---

<style>
body {
text-align: justify}
</style>

```{r setup, include=FALSE}
rm(list=ls())
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(data.table)
library(datasets)
library(MASS)
library(ISLR)
library(caret)
library(ggplot2)
library(GGally) # for ggcorr
library(NbClust)
library(skimr) # for skim function
library(factoextra) # for factoextra function
```

# 1. PCA for supervised learning
In this problem we are going to analyze the Boston dataset from the MASS package. The goal will be to predict the variable `crim` which is the crime rate.

Loading the data set:

```{r}
data <- data.table(Boston)
```

## 1.1 Do a short exploration of data and find possible predictors of the target variable.

Let's see correlations of features and the outcome variable to help quickly put together a simple benchmark model.

```{r}
ggcorr(data)
```

`rad`, `tax` and `lstat` seem to be highly correlated to `crim`, but `rad` and `tax` seem to be highly correlated to each other as well, so I am only using `rad` and `lstat`.

Having another plot to have a quick understanding of the data and its features.

```{r}
ggpairs(data)
```

## 1.2 Create a training and a test set of 50%.

```{r}
set.seed(1234)
training_ratio <- 0.5
train_indices <- createDataPartition(y = data[["crim"]],
                                     times = 1,
                                     p = training_ratio,
                                     list = FALSE)
data_train <- data[train_indices, ]
data_test <- data[-train_indices, ]
```

## 1.3 Use a linear regression to predict `crim` and use 10-fold cross validation to assess the predictive power.

Based on the above executed exploration and checking how variables are correlated to each other and to `crim`, I decided to predict using `rad` and `lstat` variables.

```{r}
set.seed(1234)
lm_fit <- train(crim ~ 
                  #zn + 
                  #indus +
                  #chas + 
                  #nox +
                  #rm +
                  #age +
                  #dis +
                  rad +
                  #tax +
                  #ptratio +
                  #black +
                  lstat,
                  #medv, 
                data = data_train, 
                method = "lm", 
                trControl = trainControl(method = "cv", number = 10)
                #preProcess = c("center", "scale")
                )
lm_fit
```

## 1.4 Try to improve the model

Try to improve the model by using PCA for dimensionality reduction. Center and scale your variables and use `pcr` to conduct a search for the optimal number of principal components. Does PCA improve the fit over the simple linear model?

`pcr` is also a linear regression but with principal components as explanatory variables and its hyperparameter is the number of principal components to be used. Now I am doing a `pcr` with a 10-fold cross-validation with a sequence of hyperparameters 1 to 13.

```{r}
tune_grid <- data.frame(ncomp = 1:13)
set.seed(1234)
pcr_fit <- train(crim ~ . , 
                data = data_train, 
                method = "pcr",
                trControl = trainControl(method = "cv", number = 10),
                tuneGrid = tune_grid,
                preProcess = c("center", "scale")
                )
pcr_fit
```

The analysis gave the lowest RMSE with a hyperparameter of 13, basically using all the variables, but the single linear model gave a lower RMSE than `pcr`.

## 1.5 Use penalized linear models

Use penalized linear models for the same task. Make sure to include lasso (`alpha = 0`) to your tune grid. How does the best model compare to that found in d)? Would pre-processing via PCA help this model? (add `pca` to `preProcess`). Why do you think the answer can be expected?

Using a penalized linear model with a 10-fold cross-validation and with a sequence of alpha values (lasso and ridge are both considered), but without principal component analysis applied in preprocessing.

```{r}
train_control <- trainControl(method = "cv",
                              number = 10)

tune_grid <- expand.grid("alpha" = seq(0 , 1, 0.1),
                         "lambda" = c(0.1, 0.01, 0.001, 0.0001))

set.seed(1234)
glmnet_model <- train(crim ~ 
                  #zn + 
                  #indus +
                  #chas + 
                  #nox +
                  #rm +
                  #age +
                  #dis +
                  rad +
                  #tax +
                  #ptratio +
                  #black +
                  lstat,
                  #medv,
                data = data_train,
                method = "glmnet",
                preProcess = c("center", "scale"),
                trControl = train_control,
                tuneGrid = tune_grid)
glmnet_model
```

Both simple and pcr linear regression gave higher RMSE than the penalized linear model.

Using the same model as above but this time applying principal component analysis (`pca`) during preprocessing. By default it filters the data to have 95% of the variance, but it can be configured, first let's experiment with the default setting. I would expect to have better results because of using `pca` for prepocessing:

```{r}
train_control <- trainControl(method = "cv",
                              number = 10)

tune_grid <- expand.grid("alpha" = seq(0 , 1, 0.1),
                         "lambda" = c(0.1, 0.01, 0.001, 0.0001))

set.seed(1234)
glmnet_pca_model <- train(crim ~ .,
                  #zn + 
                  #indus +
                  #chas + 
                  #nox +
                  #rm +
                  #age +
                  #dis +
                  #rad +
                  #tax +
                  #ptratio +
                  #black +
                  #lstat,
                  #medv,
                data = data_train,
                method = "glmnet",
                preProcess = c("center", "scale", "pca"),
                trControl = train_control,
                tuneGrid = tune_grid)
glmnet_pca_model
```

Experimenting with `pcacomp` set to 8, meaning that I set the model to limit the number of principal components to 8.

```{r}
train_control <- trainControl(method = "cv",
                              number = 10,
                              preProcOptions = list(pcaComp = 8))

tune_grid <- expand.grid("alpha" = seq(0 , 1, 0.1),
                         "lambda" = c(0.1, 0.01, 0.001, 0.0001))

set.seed(1234)
glmnet_pca_model_2 <- train(crim ~ .,
                  #zn + 
                  #indus +
                  #chas + 
                  #nox +
                  #rm +
                  #age +
                  #dis +
                  #rad +
                  #tax +
                  #ptratio +
                  #black +
                  #lstat,
                  #medv,
                data = data_train,
                method = "glmnet",
                preProcess = c("center", "scale", "pca"),
                trControl = train_control,
                tuneGrid = tune_grid)
glmnet_pca_model_2
```

My overall expectation is that using principal component analysis would provide better results by filtering the noise out.

## 1.6 Evaluate your preferred model on the test set.

Evaluating the glmnet model on test data.

```{r}
test_prediction <- predict.train(glmnet_model, 
                                        newdata = data_test)
RMSE <- function(x, true_x) sqrt(mean((x - true_x)^2))
RMSE(test_prediction, data_test[["crim"]])
```

# 2. Clustering on the USArrests dataset

In this problem use the `USArrests` dataset we used in class. Your task is to apply clustering then make sense of the clusters using the principal components.

```{r}
data <- USArrests
data <- data.table(data, keep.rownames = TRUE)
setnames(data, "rn", "state")
setnames(data, 
         c("Murder", 
           "Assault", 
           "UrbanPop",
           "Rape"), 
         c("murder", 
           "assault", 
           "urbanpop", 
           "rape"))
print(skim(data))
```

## 2.1 Determine the optimal number of clusters as indicated by `NbClust` heuristics.

`NbClust` can determine the relevant number of clusters and propose the best clustering scheme from the different results obtained by varying all combinations of number of clusters, distance measures, and clustering methods.

```{r, results="hide"}
data_features <- data[, .(murder, assault, urbanpop, rape)] # removing state variable and having only the numeric variables
nb <- NbClust(data_features, method = "kmeans", 
              min.nc = 2, max.nc = 10, index = "all") # the output to console is the most interesting
```

```{r}
fviz_nbclust(nb)
```

According to the majority rule two or three are the optimal numbers of clusters. Two has been proposed nine times, while three has been proposed six times.

## 2.2 Use the k-means method to cluster states using the number of clusters found in a) and anything else that you think that makes sense. Plot observations colored by clusters in the space of urban population and another (crime-related) variable. (See example code from class, use  `factor(km$cluster)` to create a vector of class labels).

Plotting observation in the space of urban population and murder with two clusters:

```{r}
km <- kmeans(data_features, centers = 2)
data_w_clusters <- cbind(data, 
                         data.table("cluster" = factor(km$cluster)))

ggplot(data_w_clusters, 
       aes(x = urbanpop, y = murder, color = cluster)) + 
  geom_point()
```

Plotting observation in the space of urban population and assault with two clusters:

```{r}
data_w_clusters <- cbind(data, 
                         data.table("cluster" = factor(km$cluster)))

ggplot(data_w_clusters, 
       aes(x = urbanpop, y = assault, color = cluster)) + 
  geom_point()
```

Plotting observation in the space of urban population and rape with two clusters:

```{r}
data_w_clusters <- cbind(data, 
                         data.table("cluster" = factor(km$cluster)))

ggplot(data_w_clusters, 
       aes(x = urbanpop, y = rape, color = cluster)) + 
  geom_point()
```

## 2.3 Perform PCA and get the first two principal component coordinates for all observations by

```{r}
pca_result <- prcomp(data_features, scale. = TRUE)
first_two_pc <- data.table(pca_result$x[, 1:2])
first_two_pc
```

## 2.4 Plot clusters in the coordinate system defined by the first two principal components. How do clusters relate to these?

### 2.4.1 Plotting the observations and four features / variables:

```{r}
fviz_pca(pca_result, scale = 0)
```

We can see that the first principal component captures more variance of the different crimes, while the second principal component captures rather more from the variance of `urbanpop` variable. The negative prefix does not play a role here.

### 2.4.2 Plotting the clusters:

In task 2.1 we have used `NbClust` heuristics to find the best number of clusters. Two was chosen by the model using the majority rule. All the results of `NbClust` have been loaded to list named `nb`, and it has an attribute labelling all observations into two clusters. In task 2.3 we have computed the first two principal components of all observation using PCA. Now I only had to pair the cluster labelling with the observations and their first two principal components, please find the plot below:

```{r}
data_w_clusters <- cbind(first_two_pc, 
                         data.table("cluster" = factor(nb$Best.partition)))

ggplot(data_w_clusters, 
       aes(x = PC1, y = PC2, color = cluster)) + 
  geom_point()
```

How do clusters relate to these?

PC1 seems to able to differentiate between clusters for most of the observations.

# 3. PCA of high-dimensional data

In this exercise you will perform PCA on 40 observations of 1000 variables. This is very different from what you are used to: there are much more variables than observations! These are measurments of genes of tissues of healthy and diseased patients: the first 20 observations are coming from healthy and the others from diseased patients.

Loading the data:

```{r}
data <- fread("../data/gene_data_from_ISLR_ch_10/gene_data.csv")
data[, is_diseased := factor(is_diseased)]
dim(data)
tail(names(data))
```

## 3.1 Perform PCA on this data with scaling features.

Excluding the label of being diseased when loading into a data table named `data_features`:

```{r}
data_features <- copy(data)
data_features[, is_diseased := NULL]
tail(names(data_features))
```

Performing a principal component analysis on `data_features`:

```{r}
pca_result <- prcomp(data_features, scale. = TRUE)
first_two_pc <- data.table(pca_result$x[, 1:2])
first_two_pc
```

# 3.2 Visualize datapoints in the space of the first two principal components (look at the fviz_pca_ind function). What do you see in the figure?

Visualizing all the forty observation in the space of the first two principal components:

```{r}
fviz_pca_ind(pca_result, scale = 0)
```

We can clearly see that the obsverations are divided into two groups with 20 observations in each group, we can assume that these are the group of healthy and unhealthy tissues.

To check this assumption and results I am creating a plot with using the orinial labeling:

```{r}
data_w_clusters <- cbind(first_two_pc, 
                         data.table("cluster" = factor(data$is_diseased)))

ggplot(data_w_clusters, 
       aes(x = PC1, y = PC2, color = cluster)) + 
  geom_point()
```

We can clrearly see that the clusterint was correctly done.

# 3.3 Which individual features can matter the most in separating diseased from healthy? A strategy to answer this can be the following:
+we see that PC1 matters a lot
+so look at which features have high loadings for the first PC, that is, the largest coordinates (in absolute terms). (Hint: use the $rotation).

Loading the feature names and corresponding values of variance captured by the first principal component, ordering it in a decreasing fashion and printing the largest five (considering absolute values):

```{r}
first_pc <- cbind(feature = names(data_features), data.table(abs(pca_result$rotation[,1])))
first_pc[order(V1, decreasing = TRUE)][1:5,]
```

Choose the two features with the largest coordinates and plot observations in the coordinate system defined by these two original features. What do you see?

```{r}
ggplot(data_features, 
       aes(x = measure_502, y = measure_589)) + 
  geom_point()
```

There seems to be a (perhaps linear and quadratic) relationship between the two variables, they seem to be correlated to each other, which is not surprising at all. These two features have the highest loading in the same principle component (PC1), they being correlated to each other is not suprising.

PCA thus offers a way to summarize vast amounts of variables in a handful of dimensions. This can serve as a tool to pick interesting variables where, for example, visual inspection would be hopeless.