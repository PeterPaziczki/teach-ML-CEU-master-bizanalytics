---
title: "Homework assignment 2"
subtitle: "Data Science and Machine Learning 2 - CEU 2018"
author: "Peter Paziczki"
date: '2018 Ã¡prilis 3'
output: html_document
---

<style>
body {
text-align: justify}
</style>

```{r setup, include=FALSE}
rm(list=ls())
knitr::opts_chunk$set(echo = TRUE)
```

## 0. Preparation

In the first two problems I will work with the medical appointment no-show dataset from ML1.

```{r}
library(data.table)
library(magrittr)
library(GGally)

data <- fread("../data/medical-appointments-no-show/no-show-data.csv")

# some data cleaning

## dropping columns that we don't need
data[, c("PatientId", "AppointmentID", "Neighbourhood") := NULL]

## setting the names of variables
setnames(data, 
         c("No-show", 
           "Age", 
           "Gender",
           "ScheduledDay", 
           "AppointmentDay",
           "Scholarship",
           "Hipertension",
           "Diabetes",
           "Alcoholism",
           "Handcap",
           "SMS_received"), 
         c("no_show", 
           "age", 
           "gender", 
           "scheduled_day", 
           "appointment_day",
           "scholarship",
           "hypertension",
           "diabetes",
           "alcoholism",
           "handicap",
           "sms_received"))

# for binary prediction, the target variable must be a factor variable
data[, no_show := factor(no_show, levels = c("Yes", "No"))]
data[, handicap := ifelse(handicap > 0, 1, 0)]

# creating new variables (turning them into factor variables)
data[, gender := factor(gender)]
data[, scholarship := factor(scholarship)]
data[, hypertension := factor(hypertension)]
data[, alcoholism := factor(alcoholism)]
data[, handicap := factor(handicap)]

# creating date type variables
data[, scheduled_day := as.Date(scheduled_day)]
data[, appointment_day := as.Date(appointment_day)]

# creating a new variable showing the days passed since scheduing the visit
data[, days_since_scheduled := as.integer(appointment_day - scheduled_day)]

# cleaning up a little bit
data <- data[age %between% c(0, 95)] # dropping observations with irrational ages
data <- data[days_since_scheduled > -1] # dropping observations with false days_since_scheduled values
data[, c("scheduled_day", "appointment_day", "sms_received") := NULL] # don't need scheduled_day and appointment_day variables any more
```

Please find some summary statistics below of the data set:

```{r}
skimr::skim(data)
```

```{r}
## loading h2o library
library(h2o)
h2o.init()
```

`h2o` is running on my computer now, my computer is the one and only node in the cluster right now. I need to transform the data set from a data table object into a `h2o` frame, so `h2o` can process it.

```{r}
data <- as.h2o(data)
```

## 1. Deep learning with `h2o`

### 1.1 Create train / validation / test sets, cutting the data into 5% - 45% - 50% parts.

The idea behind splitting data into different set is that When having large datasets, n-fold cross validation can be computationally intensive, hence many times train/validation/test approach is used instead. However, cross validation can still be used to tune some of the hyperparameters.

```{r}
# splitting data into three parts, train, validation and test, 5%, 45% and 50% respectively
splitted_data <- h2o.splitFrame(data, 
                                ratios = c(0.05, 0.45), # first part is 5%, second is 45%, third part is the rest
                                seed = 123)

data_train <- splitted_data[[1]]
data_valid <- splitted_data[[2]]
data_test <- splitted_data[[3]]

# making sure that the data has been split the way we wanted
nrow(data_train)
nrow(data_valid)
nrow(data_test)
```

### 1.2 Train a benchmark model of your choice using h2o (such as random forest, gbm or glm) and evaluate it on the validation set.

#### 1.2.1 Random forest as first benchmark model

```{r}
y <- "no_show"  # the outcome variable
X <- setdiff(names(data), y) # the variables we are going to use for prediction, excluding the outcome variable
```

```{r}
# random forests
rf_params <- list(ntrees = c(500), # number of trees to estimate
                  mtries = c(2, 3, 5)) # number of features that the algorithm can randomly consider at each split

rf_grid <- h2o.grid(x = X, 
                    y = y, 
                    training_frame = data_train,
                    algorithm = "randomForest", 
                    nfolds = 5,
                    seed = 123,
                    hyper_params = rf_params)

h2o.getGrid(grid_id = rf_grid@grid_id, sort_by = "rmse", decreasing = FALSE)
```

#### 1.2.2 `GBM` (gradient boosting machine) model as second benchmark model

```{r}
# GBM hyperparamters
gbm_params <- list(learn_rate = c(0.01, 0.05), # shrinkage parameter, the lower the value the smaller steps the algorithm takes
                    max_depth = c(2, 3, 5), # complexity of the trees
                    #sample_rate = c(0.5), # using only half of the data to estimate trees
                    col_sample_rate = c(0.5, 1.0)) # constraining the features to build trees, 
                                                   # 0.5 means sampling 50% of variables, 1.0 means not constraining it at all

# Train and validate a cartesian grid of GBMs
gbm_grid <- h2o.grid(x = X, 
                     y = y, 
                     training_frame = data_train, 
                     algorithm = "gbm", 
                     nfolds = 5, # doing n-fold cross-validation
                     seed = 123,
                     ntrees = 300, # number of trees to build
                     hyper_params = gbm_params)

h2o.getGrid(gbm_grid@grid_id, sort_by = "rmse", decreasing = FALSE)
```

#### 1.2.3 Validating benchmark models on validation set

```{r}
# getting the best benchmark model

# need to choose the first model, because models are sorted by RMSE in a decreasing fashion, so the first one is the best performing model
rf_model <- h2o.getModel(h2o.getGrid(rf_grid@grid_id)@model_ids[[1]])
gbm_model <- h2o.getModel(h2o.getGrid(gbm_grid@grid_id)@model_ids[[1]])

# predict on validation set
validation_performances <- list(
  #"glm" = h2o.rmse(h2o.performance(glm_model, newdata = data_valid)),
  "rf" = h2o.rmse(h2o.performance(rf_model, newdata = data_valid)),
  "gbm" = h2o.rmse(h2o.performance(gbm_model, newdata = data_valid))
)

validation_performances
```

`GBM` outperformed the random forest model, it yield an RMSE of `0.3814261`.

### 1.3 Build deep learning models. Experiment with parameter settings regarding

+ network topology (varying number of layers and nodes within layers)
+ activation function
+ dropout (both hidden and input layers)
+ lasso, ridge regularization
+ early stopping (changing stopping rounds, tolerance) and number of epochs

Present different model versions and evaluate them on the validation set. Which one performs the best?



For all the models a `validation_frame` was used, it is used to determine early stopping conditions. Supplying a validation frame helps to stop the training at the optimal parameter to avoid overfitting. In this exercise the validation set, created previously, was used as `validation_frame` and AUC was set as stopping metric.

In addition to that, option of `reproducible = TRUE` was as well, because in deep learning, for full reproducibility, setting the seed is not enough. It does not utilize everything it could, if setting it to TRUE.

### 1.3.1 Single DL model

```{r}
dl_model <- h2o.deeplearning(x = X, 
                             y = y, 
                             training_frame = data_train, 
                             validation_frame = data_valid, # supplying a validation frame as an input
                             # there is a so called early stopping parameter ... we can tell the optimization process to stop
                             # whenever we see fit ... what are these measures?
                             # the validation frame helps us to stop at the optimal parameter to avoid overfitting
                             reproducible = TRUE,  # makes training slower but makes it reproducible
                             seed = 123)

# the score history of models trained
h2o.scoreHistory(dl_model)
```

### 1.3.2 Experimenting with network structure and functional form

The attributum of `hidden` refers to neuron layer architecture, the length of vector shows number of layers, the value shows the actual number of neurons within a layer. The default setting is two hidden layers with 200-200 neurons (see exercise 1.3.1).

First I am experimenting with one hidden layer, with 10 on 512 neuron on it.

`shallow_small_model` with 10 neurons on the hidden layer:

```{r}
shallow_small_model <- h2o.deeplearning(x = X, 
                             y = y, 
                             training_frame = data_train, 
                             validation_frame = data_valid,
                             hidden = c(10), # the length of the vector tells the number of layers and the value is the number of neurons
                             # within that layer, in this case we have one number, so there is only one hidden layer, the value is 10, so the
                             # hidden layer will have 10 neurons
                             seed = 123)
```

`shallow_large_model` with 512 neurons on the hidden layer:

```{r}
shallow_large_model <- h2o.deeplearning(x = X, 
                             y = y, 
                             training_frame = data_train, 
                             validation_frame = data_valid,
                             hidden = c(512),
                             seed = 123)
```

Now I am experimenting with more layers.

`deep_small_model` four hidden layers with 32 neurons on each layer:

```{r}
deep_small_model <- h2o.deeplearning(x = X, 
                             y = y, 
                             training_frame = data_train, 
                             validation_frame = data_valid,
                             hidden = c(32, 32, 32, 32, 32), # we have five layers with 32 neurons each
                             seed = 123)
```

`deep_large_model` with three hidden layers with 100 neurons on each

```{r}
deep_large_model <- h2o.deeplearning(x = X, 
                             y = y, 
                             training_frame = data_train, 
                             validation_frame = data_valid,
                             hidden = c(100, 100, 100),
                             seed = 123)
```

### 1.3.3 Experimenting with activitation function

The activation function is a parametert that can be set, the default is rectified linear function, but could change it to many other functions. Now I am experimenting with the Tanh, a tangent hyperbolic activation function, modifying the `deep_small_model` model.

The job of activation function is, after linearly combining the inputs and their weights, to create a neuron with a valie between zero and one.

```{r}
tanh_model <- h2o.deeplearning(x = X, 
                 y = y, 
                 training_frame = data_train, 
                 validation_frame = data_valid,
                 hidden = c(32, 32, 32, 32, 32),
                 activation = "Tanh", # default is rectified linear, we can change to many other function, tanh for instance
                 # (tanh is tangent hyperbolic)
                 # after linearly combining the inputs and their weights, the function on them to create the neuron an be changed
                 seed = 123)
```

### 1.3.4 Experimenting with `epochs`

It can be set, how many times all training data points would be used to adjust the model in the course of optimization, it is called `epoch`. It is important to note that if early stopping criteria were set, the would be no guarantee to use all `epoch`. In this experience it is set to 200, early stopping is set to default and structure is from model `deep_small_model` with four hidden layers and 32 neurons on each layer. This structure will be used in all the coming models.

```{r}
more_epochs_model <- h2o.deeplearning(x = X, 
                 y = y, 
                 training_frame = data_train, 
                 validation_frame = data_valid,
                 hidden = c(32, 32, 32, 32, 32),
                 epochs = 200,
                 stopping_rounds = 0, # setting it to zero means there is no early stopping applied
                 seed = 123)
```

### 1.3.5 Experimenting with `batch`

It can be set that after how many training samples is the gradient update made, it is called `mini_batch_size`, default is one, not I am experimenting with ten.

```{r}
higher_batch_size_model <- h2o.deeplearning(x = X, 
                 y = y, 
                 training_frame = data_train, 
                 validation_frame = data_valid,
                 hidden = c(32, 32, 32, 32, 32),
                 mini_batch_size = 10, # increasing it will yield less precise results, but will be faster
                 seed = 123)
```

### 1.3.6 Experimenting with regularization

With attribute called `hidden_dropout_ratios` it can be set that with how large probability will neurons be left out of the model at a step, default is 0.5. There is a possibility to set different values to each layer. To use this kind of regularization a special activitation function is to set, called `WithDropout`, or in this case `RectifierWithDropout`.

```{r}
dropout_model <- h2o.deeplearning(x = X, 
                 y = y, 
                 training_frame = data_train, 
                 validation_frame = data_valid,
                 hidden = c(32, 32, 32, 32, 32),
                 activation = "RectifierWithDropout", # dropping out some nodes randomly by using an activation function
                 hidden_dropout_ratios = c(0.1, 0.1, 0.2, 0.2, 0.2), # a vector can also be used, length of vector equals to the number
                 # of hidden layers, each number tells what percent of nodes on that specific layer is to drop out randomly
                 seed = 123)
```

Another option to regularization is use attribute `input_dropout_ratio`, by using it some input features can be dropped randomly from the input layer. In this experiment it is set to 0.4.

```{r}
input_dropout_model <- h2o.deeplearning(x = X, 
                 y = y, 
                 training_frame = data_train, 
                 validation_frame = data_valid,
                 hidden = c(32, 32, 32, 32, 32),
                 input_dropout_ratio = 0.4, # the above showed dropout technique can be applied to the input layer as well
                 seed = 123)
```

### 1.3.7 Experimenting with Lasso and Ridge regularization

Another option to do regularization is doing Lasso and Ridge by using attributes `l1` and `l2`, weight on $L1$ (lasso) and $L2$ (ridge) penalty terms.

```{r}
regularized_model <- h2o.deeplearning(x = X, 
                 y = y, 
                 training_frame = data_train, 
                 validation_frame = data_valid,
                 hidden = c(32, 32, 32, 32, 32),
                 # NN is a highly parameterized model, every weight is already a parameter
                 l1 = 0.001,
                 l2 = 0.001,
                 seed = 123)
```

### 1.3.8 Experimenting with early stopping options

Training constantly tracks validation frame performance. Early stopping is enabled by default but can be tuned when to stop. This, again, is to prevent overfitting. (If one does not supply a `validation_frame`, early stopping still works but based on metrics calculated from the training set = may not be as informative for out of sample performance.) There are early stopping options, such as `stopping_rounds`, `stoppnig_metric` and `stopping_tolerance`. In this experiment `stopping_rounds` is set to two, `stoppnig_metric` is `AUC` and `stopping_tolerance` is 0.01.

```{r}
early_stopping_model <- h2o.deeplearning(x = X, 
                 y = y, 
                 training_frame = data_train, 
                 validation_frame = data_valid, # at each update of the frame, the performance of the current state of the model is validated
                 # on the validation set, if the performance stops increasing, we stop the training
                 # with other words, the model we are building is not only validated at the end of training, but at each steps it is checked
                 # to see that it still worth to continue
                 hidden = c(32, 32, 32, 32, 32),
                 epochs = 100,
                 stopping_rounds = 2, # moving average is calculated, it takes the average of the last and previous results (default is five)
                 stopping_metric = "AUC", # metric of performance
                 stopping_tolerance = 0.01, # how much increase in AUC is need to continue the training process
                 seed = 123)
```

### 1.4 How does your best model compare to the benchmark model on the test set?

```{r}
# validation set performance

# checking performance of benchmark models and NNs on validation set
validation_performances <- list(
  "benchmark_rf" = h2o.performance(rf_model, data_valid)@metrics$AUC,
  "benchmark_gbm" = h2o.performance(gbm_model, data_valid)@metrics$AUC,
  "benchmark_dl" = h2o.performance(dl_model, data_valid)@metrics$AUC,
  "shallow_small_model" = h2o.performance(shallow_small_model, data_valid)@metrics$AUC,
  "shallow_large_model" = h2o.performance(shallow_large_model, data_valid)@metrics$AUC,
  "deep_small_model" = h2o.performance(deep_small_model, data_valid)@metrics$AUC,
  "deep_large_model" = h2o.performance(deep_large_model, data_valid)@metrics$AUC,
  "tanh_model" = h2o.performance(tanh_model, data_valid)@metrics$AUC,
  "more_epochs_model" = h2o.performance(more_epochs_model, data_valid)@metrics$AUC,
  "higher_batch_size_model" = h2o.performance(higher_batch_size_model, data_valid)@metrics$AUC,
  "dropout_model" = h2o.performance(dropout_model, data_valid)@metrics$AUC,
  "input_dropout_model" = h2o.performance(input_dropout_model, data_valid)@metrics$AUC,
  "regularized_model" = h2o.performance(regularized_model, data_valid)@metrics$AUC,
  "early_stopping_model" = h2o.performance(early_stopping_model, data_valid)@metrics$AUC
)

validation_performances
```

The `deep_large_model` had the best performance among the neural nets, but the `benchmark_gbm` outperformed it. So summarizing it, neural nets could outperform random forest in many cases, but gradient boosting machine model was far the best over neural nets.

### 1.5 Evaluate the model that performs best based on the validation set on the test set.

The best performing model on the validation set, of all models, including the becnhmark models too, was the `benchmark_gbm` benchmark model with AUC of `0.7201964`. Its performance on the test is close to the previous result, it was `AUC = 0.7187`.

```{r}
h2o.performance(gbm_model, data_valid)@metrics$AUC
h2o.performance(gbm_model, data_test)@metrics$AUC
```

The best performing neural net model on the validation set was the `tanh_model` with `AUC = 0.712112` and had a result of `AUC = 0.7115219` on the test set. This the model where I was experimenting with the activitation function.

```{r}
h2o.performance(tanh_model, data_valid)@metrics$AUC
h2o.performance(tanh_model, data_test)@metrics$AUC
```

## 2. Stacking with `h2o`

Take the same problem and data splits.

### 2.1 Build at least 4 models of different families using cross validation, keeping cross validated predictions.

#### 2.1.1 I am going to build different base learners, starting with a `GBM` model, building 300 trees, using 5-fold cross-validation and keeping cross validated predictions, setting `max_depth` to 5, `learn_rate` to 0.1 and `col_sample_rate` to 0.5.

```{r}
gbm_model <- h2o.gbm(
  X, y,
  training_frame = data_train,
  ntrees = 300, 
  max_depth = 5, 
  learn_rate = 0.1,
  col_sample_rate = 0.5,
  seed = 123,
  nfolds = 5, 
  keep_cross_validation_predictions = TRUE # the scores are the cross validation predictions
  # score is a predicted probability that the prediction is true
)
```

#### 2.1.2 More complex `GBM` model using Grid support

I am still building a `GBM` model, but this time a more complex one by less constraining the training.

```{r}
gbm_model_complex <- h2o.gbm(
  X, y,
  training_frame = data_train,
  ntrees = 500, 
  max_depth = 13, 
  learn_rate = 0.05, 
  col_sample_rate = 0.8,
  seed = 123,
  nfolds = 5, 
  keep_cross_validation_predictions = TRUE # the scores are the cross validation predictions
  # score is a predicted probability that the prediction is true
)
```

#### 2.1.3 Deep learning model

I am building a very simple deep learning model, having only two hidden layers, with 32 and 8 nodes on the layers.

```{r}
deeplearning_model <- h2o.deeplearning(
  X, y,
  training_frame = data_train,
  hidden = c(32, 8),
  seed = 123,
  nfolds = 5, 
  keep_cross_validation_predictions = TRUE
)
```

#### 2.1.4 More complex deep learning model

Now I am building a more complex deep learning model, having three hidden layers with 100 neurons on each, having a `Tanh` activitation function and setting `epochs` to 50.

```{r}
deeplearning_model_complex <- h2o.deeplearning(
  X, y,
  training_frame = data_train,
  hidden = c(100, 100, 100),
  seed = 123,
  nfolds = 5,
  activation = "Tanh",
  epochs = 50,
  keep_cross_validation_predictions = TRUE
)
```

#### 2.1.5 Random forest model

I am building a simple random forest model, building 300 trees, trying different values for `mtries` (two and three), and setting `min_split_improvement` to 0,001.

```{r}
randomforest_model <- h2o.randomForest(
  X, y,
  training_frame = data_train,
  seed = 123,
  nfolds = 5, 
  ntrees = 300,
  mtries = 3,
  min_split_improvement = 1e-03,
  keep_cross_validation_predictions = TRUE
)
```

#### 2.1.6 Random forest model

I am building a more complex random forest model, building 500 trees, trying different values for `mtries` (five and eight), and setting `min_split_improvement` to 0,00001.

```{r}
randomforest_model_complex <- h2o.randomForest(
  X, y,
  training_frame = data_train,
  seed = 123,
  nfolds = 5, 
  ntrees = 500,
  mtries = 8,
  min_split_improvement = 1e-05,
  keep_cross_validation_predictions = TRUE
)
```

#### 2.1.7 `XGBoost` model

I was trying to build an `XGBoost` model, but unfortunately I couldn't. The `h2o.xgboost.available()` function asks the H2O server whether a `XGBoost` model can be built (depends on availability of native backend). Returns True if a such a model can be built and False otherwise. It returned False in my case.

```{r}
h2o.xgboost.available()
```

### 2.2 Evaluate validation set performance of each model.

```{r}
# validation set performance

# checking performance of base learner models on validation set
validation_performances <- list(
  "gbm_model" = h2o.performance(gbm_model, data_valid)@metrics$AUC,
  "gbm_model_complex" = h2o.performance(gbm_model_complex, data_valid)@metrics$AUC,
  "deeplearning_model" = h2o.performance(deeplearning_model, data_valid)@metrics$AUC,
  "deeplearning_model_complex" = h2o.performance(deeplearning_model_complex, data_valid)@metrics$AUC,
  "randomforest_model" = h2o.performance(randomforest_model, data_valid)@metrics$AUC,
  "randomforest_model_complex" = h2o.performance(randomforest_model_complex, data_valid)@metrics$AUC
)

validation_performances
```

```{r, eval = FALSE, echo = FALSE}
# validation set performance - another way to evaluate it

# checking performance of base learner models on validation set
validation_performances <- list(
  "gbm_model" = print(h2o.auc(h2o.performance(gbm_model, newdata = data_valid))),
  "gbm_model_complex" = print(h2o.auc(h2o.performance(gbm_model_complex, newdata = data_valid))),
  "deeplearning_model" = print(h2o.auc(h2o.performance(deeplearning_model, newdata = data_valid))),
  "deeplearning_model_complex" = print(h2o.auc(h2o.performance(deeplearning_model_complex, newdata = data_valid))),
  "randomforest_model" = print(h2o.auc(h2o.performance(randomforest_model, newdata = data_valid))),
  "randomforest_model_complex" = print(h2o.auc(h2o.performance(randomforest_model_complex, newdata = data_valid)))
)
validation_performances
```

### 2.3 How large are the correlations of predicted scores of the validation set produced by the base learners?

Below I am inspecting, to what extent are predicted scores correlated. It is not surprising that they are correlated, they all try to predict the same thing, but still there are scores that have a low correlation, for example the `deeplearning_model` and the `randomforest_model_complex` models, they are the least correlated. The most correlated score were produced by `randomforest_model` and `randomforest_model_complex` models, though their performance is quite far from each other. It is very interesting that `deeplearning_model` and `deeplearning_model_complex` models are not strongly correlated at all, though their performance is close to each other.

```{r}
# inspect test set correlations of scores
predictions <- data.table(
  "gbm" = as.data.frame(h2o.predict(gbm_model, newdata = data_test)$Y)$Y,
  "gbm_complex" = as.data.frame(h2o.predict(gbm_model_complex, newdata = data_test)$Y)$Y,
  "dl" = as.data.frame(h2o.predict(deeplearning_model, newdata = data_test)$Y)$Y,
  "dl_complex" = as.data.frame(h2o.predict(deeplearning_model_complex, newdata = data_test)$Y)$Y,
  "rf" = as.data.frame(h2o.predict(randomforest_model, newdata = data_test)$Y)$Y,
  "rf_complex" = as.data.frame(h2o.predict(randomforest_model_complex, newdata = data_test)$Y)$Y
)

ggcorr(predictions, label = TRUE, label_round = 2)
```

### 2.4 Create a stacked ensemble model from the base learners. Experiment with at least two different ensembling meta learners.

#### 2.4.1 `GLM` model as meta learner

The default meta learner is a `GLM` model, now I am tranining a `GLM` meta learner with default settings.

```{r}
ensemble_model_glm <- h2o.stackedEnsemble( # built in h2o function, it estimates the meta model
                                       # on the scores estimated by the previously trained models
  X, y,
  training_frame = data_train,
  base_models = list(gbm_model, 
                     gbm_model_complex,
                     deeplearning_model,
                     deeplearning_model_complex,
                     randomforest_model,
                     randomforest_model_complex))
```

#### 2.4.2 `GBM` model as meta learner

Now I am experimenting with `GBM` as the meta model, with default parameters.

```{r}
ensemble_model_gbm <- h2o.stackedEnsemble(
  X, y,
  training_frame = data_train,
  metalearner_algorithm = "gbm",
  base_models = list(gbm_model, 
                     gbm_model_complex,
                     deeplearning_model,
                     deeplearning_model_complex,
                     randomforest_model,
                     randomforest_model_complex))
```

#### 2.4.3 `drf` model as meta learner

Now I am experimenting with `drf` (random forest) as the meta model, with default parameters.

```{r}
ensemble_model_drf <- h2o.stackedEnsemble(
  X, y,
  training_frame = data_train,
  metalearner_algorithm = "drf",
  base_models = list(gbm_model, 
                     gbm_model_complex,
                     deeplearning_model,
                     deeplearning_model_complex,
                     randomforest_model,
                     randomforest_model_complex))
```

#### 2.4.4 `deeplearning` model as meta learner

Now I am experimenting with `deeplearning` as the meta model, with default parameters.

```{r}
ensemble_model_dl <- h2o.stackedEnsemble(
  X, y,
  training_frame = data_train,
  metalearner_algorithm = "deeplearning",
  base_models = list(gbm_model, 
                     gbm_model_complex,
                     deeplearning_model,
                     deeplearning_model_complex,
                     randomforest_model,
                     randomforest_model_complex))
```

### 2.5 Evaluate ensembles on validation set. Did it improve prediction?

Using random forest as meta learner had the lowest performance. `GBM` as meta learner had better results compared to random forest, but couldn't outperform a single `GBM` model. Deep learning as a meta model outperformed all the single models and the two meta models I just mentioned. The best performing meta model was the `GLM` with AUC = `0.7131943`. 

```{r}
# validation set performance

# checking performance of ensemble models on validation set
validation_performances <- list(
  "glm" = print(h2o.auc(h2o.performance(ensemble_model_glm, newdata = data_valid))),
  "gbm" = print(h2o.auc(h2o.performance(ensemble_model_gbm, newdata = data_valid))),
  "rf" = print(h2o.auc(h2o.performance(ensemble_model_drf, newdata = data_valid))),
  "dl" = print(h2o.auc(h2o.performance(ensemble_model_dl, newdata = data_valid)))
)

validation_performances
```

### 2.6 Evaluate the best performing model on the test set. How does performance compare to that of the validation set?

As `GLM` was the best meta learner, I validated it on the test set and had a performance of AUC = `0.7117571`. It is quite close to the results of the validation on the validation set.

```{r}
print(h2o.auc(h2o.performance(ensemble_model_glm, newdata = data_test)))
```

### 2.7 Extra experiment

Meta-learning can also be built upon same-family, different hyperparameter models via a grid of hyperparameters. I am experimenting with a `GBM` model. First I am building models with different parameters for `learn_rate` and `max_depth`.

```{r}
learn_rate_opt <- c(0.1, 0.3)
max_depth_opt <- c(3, 5, 7) # we can use cross-validation to choose the best hyperparameter
hyper_params <- list(learn_rate = learn_rate_opt,
                     max_depth = max_depth_opt)

gbm_grid <- h2o.grid(
  x = X, y = y,
  training_frame = data_train,
  algorithm = "gbm",
  ntrees = 100,
  hyper_params = hyper_params,
  seed = 123,
  nfolds = 5, 
  keep_cross_validation_predictions = TRUE)
```

Now I am checking the performance of these models on the test set.

```{r}
# individual test set performances
for (model_id in gbm_grid@model_ids) {
  model <- h2o.getModel(model_id)
  print(model_id)
  print(h2o.auc(h2o.performance(model, newdata = data_test)))
}

# individually none of them reached 0.72, but by stacking them we could improve that
```

Now I am stacking these models.

```{r}
ensemble_model_grid_gbm <- h2o.stackedEnsemble(
  X, y,
  training_frame = data_train,
  base_models = gbm_grid@model_ids)
```

Now I am evaluating the performance of the stacked model on the test set.

```{r}
print(h2o.auc(h2o.performance(ensemble_model_grid_gbm, newdata = data_test)))
# by stacking we improved the model 
```

The stacked model could outperform all the individual models. I noted that the one of the individual models was pretty close to the stacked, but it does not apply to the rest.

## 2. Fashion image classification using `keras`

Take the âFashion MNIST datasetâ where images of fashion items are to be classified in a similar manner to what we saw with handwritten digits (see more here). Images are in exactly the same format as we saw digits: 28x28 pixel grayscale images. The task is to build deep neural net models to predict image classes. The goal is to have as accurate classifier as possible: we are using accuracy as a measure of predictive power.

```{r}
library(keras)
fashion_mnist <- dataset_fashion_mnist()
x_train <- fashion_mnist$train$x
y_train <- fashion_mnist$train$y
x_test <- fashion_mnist$test$x
y_test <- fashion_mnist$test$y
```

### 3.1 Show some example images from the data.

We are going to classify fashion items, it is an example of multiclass classification. The train data set has 60.000 images with a resolution of 28x28 pixels. We have 10.000 images in the test with the same resolution. We have the following labels for fashion items:

+ 0 T-shirt/top
+ 1 Trouser
+ 2 Pullover
+ 3 Dress
+ 4 Coat
+ 5 Sandal
+ 6 Shirt
+ 7 Sneaker
+ 8 Bag
+ 9 Ankle boot 

```{r, fig.width=2, fig.height=2}
show_mnist_image <- function(x) {
  image(1:28, 1:28, t(x)[,nrow(x):1],col=gray((0:255)/255)) 
}

# showing some example images from the training data

show_mnist_image(x_train[18, , ])
y_train[18] # the GT - ground truth - it is a T-shirt/top

show_mnist_image(x_train[19, , ])
y_train[19] # it is a shirt

show_mnist_image(x_train[20, , ])
y_train[20] # coat

show_mnist_image(x_train[21, , ])
y_train[21] # dress

show_mnist_image(x_train[22, , ])
y_train[22] # trouser
```

### 3.2 Train a fully connected deep network to predict items.

+ Normalize the data similarly to what we saw with MNIST.

+ Experiment with network architectures and settings (number of hidden layers, number of nodes, activation functions, etc.)

+ Explain what you have tried, what worked and what did not. Present a final model.

+ Make sure that you use enough epochs so that the validation error starts flattening out - provide a plot about the training history (  plot(history))

I need to reshape and rescale the data so I could work with it. First I need to create vectors of each images. As the resolution is 28x28 pixels, length of each vector will be 28x28 = 784. Every value in the vector represents a pixel, with a number between 0 and 255, represing a shade of grey. I need to rescale them, so they would be between 0 and 1.

```{r}
# reshaping images
x_train <- array_reshape(x_train, c(dim(x_train)[1], 784)) # creating vector with a length of 784 features of matrices sized 28x28
# every pixel is a feature
x_test <- array_reshape(x_test, c(dim(x_test)[1], 784)) 

# rescaling
x_train <- x_train / 255 # resclaing it so it can be between 0 and 1
x_test <- x_test / 255

# one-hot encoding of the target variable
y_train <- to_categorical(y_train, 10) # it is a Keras function, this is the way that Keras accepts outcome variables
y_test <- to_categorical(y_test, 10)
```

### 3.2.1 Benchmark model

First I am going to build a benchmark and will try to inprove it. 

First step is to define the model. The core data structure of Keras is a model, a way to organize layers. The simplest type of model is the Sequential model, a linear stack of layers. I begin by creating a sequential model and then adding layers using the pipe (%>%) operator.

```{r}
model <- keras_model_sequential() # sequental, because we go layer by layer
model %>% 
  layer_dense(units = 128, activation = 'relu', input_shape = c(784)) %>%
  layer_dropout(rate = 0.3) %>%
  layer_dense(units = 10, activation = 'softmax')
```

The `input_shape` argument to the first layer specifies the shape of the input data (a length 784 numeric vector representing a grayscale image). The final layer outputs a length 10 numeric vector (probabilities for each digit) using a softmax activation function.

By using the `summary()` function I can print the details of the model, please find it below.

```{r}
summary(model)
```

Next step is to compile the model with:

+ an appropriate loss function, `categorical_crossentropy` in this case,

+ an optimizer, this time it `optimizer_rmsprop()`, it divides the gradient by a running average of its recent magnitude. It is recommended to leave the parameters of this optimizer at their default values (except the learning rate, which can be freely tuned).

+ and metrics, `accuracy` was chosen in this exercise.

```{r}
model %>% compile(
  loss = 'categorical_crossentropy', # defining a loss function
  optimizer = optimizer_rmsprop(),
  metrics = c('accuracy') # chossing metrics
)
```

The next step is training and evaluation. I need to use the `fit()` function to train the model for 30 epochs using batches of 128 images:

```{r}
history <- model %>% fit(
  x_train, y_train, # defining the training and the outcome data-sets
  epochs = 30, batch_size = 128, # batc_size defines the number of samples that are going to be propagated through the network
  validation_split = 0.2 # 20% of training set will be set aside for validation
)
```

The charts are being drawn when building the model. The upper chart is the loss function, the bottom one is about the number of echos. The more echos we have, the better results we can achieve, at least this is what the it suggests. The accuracy we could reach was `0.9108`. I will try to inprove it in the next section.

The `history` object returned by `fit()` includes loss and accuracy metrics which I am plotting below:

```{r}
plot(history)
```

### 3.2.2 Inproved model

Now I am trying to improve the model by adding another layer to it and experimenting with the network architecture and settings. In this new layer I am setting:

+ the `units` in `layer_dense` to 256, it is the dimensionality of the output space;

+ the rate of `layer_dropout` to 0.4, it is the fraction of the input units to drop randomly;

```{r}
model <- keras_model_sequential() 
model %>% 
  layer_dense(units = 256, activation = 'relu', input_shape = c(784)) %>% 
  layer_dropout(rate = 0.4) %>% 
  layer_dense(units = 128, activation = 'relu') %>%
  layer_dropout(rate = 0.3) %>%
  layer_dense(units = 10, activation = 'softmax')
```

We can see that we have more layers.

```{r}
summary(model)
```

```{r}
model %>% compile(
  loss = 'categorical_crossentropy',
  optimizer = optimizer_rmsprop(),
  metrics = c('accuracy')
)
```

```{r}
history <- model %>% fit(
  x_train, y_train, # defining the training and the outcome data-sets
  epochs = 100, batch_size = 128, 
  validation_split = 0.2 # 20% of training set will be set aside for validation
)
```

The `history` object returned by `fit()` includes loss and accuracy metrics which I am plotting below:

```{r}
plot(history)
```

### 3.3 Evaluate the model on the test set. How does test error compare to validation error?

```{r}
model %>% evaluate(x_test, y_test)
```

### 3.4 Try building a convolutional neural network and see if you can improve test set performance.