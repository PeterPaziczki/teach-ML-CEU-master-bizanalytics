---
title: "Homework assignment 2"
subtitle: "Data Science and Machine Learning 2 - CEU 2018"
author: "Peter Paziczki"
date: '2018 április 3'
output: html_document
---

<style>
body {
text-align: justify}
</style>

```{r setup, include=FALSE}
rm(list=ls())
knitr::opts_chunk$set(echo = TRUE)
```

## 0. Preparation

In the first two problems I will work with the medical appointment no-show dataset from ML1.

```{r}
library(data.table)
library(magrittr)

data <- fread("../data/medical-appointments-no-show/no-show-data.csv")

# some data cleaning

## dropping columns that we don't need
data[, c("PatientId", "AppointmentID", "Neighbourhood") := NULL]

## setting the names of variables
setnames(data, 
         c("No-show", 
           "Age", 
           "Gender",
           "ScheduledDay", 
           "AppointmentDay",
           "Scholarship",
           "Hipertension",
           "Diabetes",
           "Alcoholism",
           "Handcap",
           "SMS_received"), 
         c("no_show", 
           "age", 
           "gender", 
           "scheduled_day", 
           "appointment_day",
           "scholarship",
           "hypertension",
           "diabetes",
           "alcoholism",
           "handicap",
           "sms_received"))

# for binary prediction, the target variable must be a factor variable
data[, no_show := factor(no_show, levels = c("Yes", "No"))]
data[, handicap := ifelse(handicap > 0, 1, 0)]

# creating new variables (turning them into factor variables)
data[, gender := factor(gender)]
data[, scholarship := factor(scholarship)]
data[, hypertension := factor(hypertension)]
data[, alcoholism := factor(alcoholism)]
data[, handicap := factor(handicap)]

# creating date type variables
data[, scheduled_day := as.Date(scheduled_day)]
data[, appointment_day := as.Date(appointment_day)]

# creating a new variable showing the days passed since scheduing the visit
data[, days_since_scheduled := as.integer(appointment_day - scheduled_day)]

# cleaning up a little bit
data <- data[age %between% c(0, 95)] # dropping observations with irrational ages
data <- data[days_since_scheduled > -1] # dropping observations with false days_since_scheduled values
data[, c("scheduled_day", "appointment_day", "sms_received") := NULL] # don't need scheduled_day and appointment_day variables any more
```

Please find some summary statistics below of the data set:

```{r}
skimr::skim(data)
```

```{r}
## loading h2o library
library(h2o)
h2o.init()
```

`h2o` is running on my computer now, my computer is the one and only node in the cluster right now. I need to transform the data set from a data table object into a `h2o` frame, so `h2o` can process it.

```{r}
data <- as.h2o(data)
```

## 1. Deep learning with `h2o`

### 1.1 Create train / validation / test sets, cutting the data into 5% - 45% - 50% parts.

The idea behind splitting data into different set is that When having large datasets, n-fold cross validation can be computationally intensive, hence many times train/validation/test approach is used instead. However, cross validation can still be used to tune some of the hyperparameters.

```{r}
# splitting data into three parts, train, validation and test, 5%, 45% and 50% respectively
splitted_data <- h2o.splitFrame(data, 
                                ratios = c(0.05, 0.45), # first part is 5%, second is 45%, third part is the rest
                                seed = 123)

data_train <- splitted_data[[1]]
data_valid <- splitted_data[[2]]
data_test <- splitted_data[[3]]

# making sure that the data has been split the way we wanted
nrow(data_train)
nrow(data_valid)
nrow(data_test)
```

### 1.2 Train a benchmark model of your choice using h2o (such as random forest, gbm or glm) and evaluate it on the validation set.

#### 1.2.1 Random forest as first benchmark model

```{r}
y <- "no_show"  # the outcome variable
X <- setdiff(names(data), y) # the variables we are going to use for prediction, excluding the outcome variable

# random forests
rf_params <- list(ntrees = c(500), # number of trees to estimate
                  mtries = c(2, 3, 5)) # number of features that the algorithm can randomly consider at each split

rf_grid <- h2o.grid(x = X, 
                    y = y, 
                    training_frame = data_train,
                    algorithm = "randomForest", 
                    nfolds = 5,
                    seed = 123,
                    hyper_params = rf_params)

h2o.getGrid(grid_id = rf_grid@grid_id, sort_by = "rmse", decreasing = FALSE)
```

#### 1.2.2 `GBM` (gradient boosting machine) model as second benchmark model

```{r}
# GBM hyperparamters
gbm_params <- list(learn_rate = c(0.01, 0.05), # shrinkage parameter, the lower the value the smaller steps the algorithm takes
                    max_depth = c(2, 3, 5), # complexity of the trees
                    #sample_rate = c(0.5), # using only half of the data to estimate trees
                    col_sample_rate = c(0.5, 1.0)) # constraining the features to build trees, 
                                                   # 0.5 means sampling 50% of variables, 1.0 means not constraining it at all

# Train and validate a cartesian grid of GBMs
gbm_grid <- h2o.grid(x = X, 
                     y = y, 
                     training_frame = data_train, 
                     algorithm = "gbm", 
                     nfolds = 5, # doing n-fold cross-validation
                     seed = 123,
                     ntrees = 300, #♀ number of trees to build
                     hyper_params = gbm_params)

h2o.getGrid(gbm_grid@grid_id, sort_by = "rmse", decreasing = FALSE)
```

#### 1.2.3 Validating benchmark models on validation set

```{r}
# getting the best benchmark model

# need to choose the first model, because models are sorted by RMSE in a decreasing fashion, so the first one is the best performing model
rf_model <- h2o.getModel(h2o.getGrid(rf_grid@grid_id)@model_ids[[1]])
gbm_model <- h2o.getModel(h2o.getGrid(gbm_grid@grid_id)@model_ids[[1]])

# predict on validation set
validation_performances <- list(
  #"glm" = h2o.rmse(h2o.performance(glm_model, newdata = data_valid)),
  "rf" = h2o.rmse(h2o.performance(rf_model, newdata = data_valid)),
  "gbm" = h2o.rmse(h2o.performance(gbm_model, newdata = data_valid))
)

validation_performances
```

`GBM` outperformed the random forest model, it yield an RMSE of `0.3814261`.

### 1.3 Build deep learning models. Experiment with parameter settings regarding

+ network topology (varying number of layers and nodes within layers)
+ activation function
+ dropout (both hidden and input layers)
+ lasso, ridge regularization
+ early stopping (changing stopping rounds, tolerance) and number of epochs

Present different model versions and evaluate them on the validation set. Which one performs the best?



For all the models a `validation_frame` was used, it is used to determine early stopping conditions. Supplying a validation frame helps to stop the training at the optimal parameter to avoid overfitting. In this exercise the validation set, created previously, was used as `validation_frame` and AUC was set as stopping metric.

In addition to that, option of `reproducible = TRUE` was as well, because in deep learning, for full reproducibility, setting the seed is not enough. It does not utilize everything it could, if setting it to TRUE.

### 1.3.1 Single DL model

```{r}
dl_model <- h2o.deeplearning(x = X, 
                             y = y, 
                             training_frame = data_train, 
                             validation_frame = data_valid, # supplying a validation frame as an input
                             # there is a so called early stopping parameter ... we can tell the optimization process to stop
                             # whenever we see fit ... what are these measures?
                             # the validation frame helps us to stop at the optimal parameter to avoid overfitting
                             reproducible = TRUE,  # makes training slower but makes it reproducible
                             seed = 123)

# the score history of models trained
h2o.scoreHistory(dl_model)
```

### 1.3.2 Experimenting with network structure and functional form

The attributum of `hidden` refers to neuron layer architecture, the length of vector shows number of layers, the value shows the actual number of neurons within a layer. The default setting is two hidden layers with 200-200 neurons (see exercise 1.3.1).

First I am experimenting with one hidden layer, with 10 on 512 neuron on it.

`shallow_small_model` with 10 neurons on the hidden layer:

```{r}
shallow_small_model <- h2o.deeplearning(x = X, 
                             y = y, 
                             training_frame = data_train, 
                             validation_frame = data_valid,
                             hidden = c(10), # the length of the vector tells the number of layers and the value is the number of neurons
                             # within that layer, in this case we have one number, so there is only one hidden layer, the value is 10, so the
                             # hidden layer will have 10 neurons
                             seed = 123)
```

`shallow_large_model` with 512 neurons on the hidden layer:

```{r}
shallow_large_model <- h2o.deeplearning(x = X, 
                             y = y, 
                             training_frame = data_train, 
                             validation_frame = data_valid,
                             hidden = c(512),
                             seed = 123)
```

Now I am experimenting with more layers.

`deep_small_model` four hidden layers with 32 neurons on each layer:

```{r}
deep_small_model <- h2o.deeplearning(x = X, 
                             y = y, 
                             training_frame = data_train, 
                             validation_frame = data_valid,
                             hidden = c(32, 32, 32, 32, 32), # we have five layers with 32 neurons each
                             seed = 123)
```

`deep_large_model` with three hidden layers with 100 neurons on each

```{r}
deep_large_model <- h2o.deeplearning(x = X, 
                             y = y, 
                             training_frame = data_train, 
                             validation_frame = data_valid,
                             hidden = c(100, 100, 100),
                             seed = 123)
```

### 1.3.3 Experimenting with activitation function

The activation function is a parametert that can be set, the default is rectified linear function, but could change it to many other functions. Now I am experimenting with the Tanh, a tangent hyperbolic activation function, modifying the `deep_small_model` model.

The job of activation function is, after linearly combining the inputs and their weights, to create a neuron with a valie between zero and one.

```{r}
tanh_model <- h2o.deeplearning(x = X, 
                 y = y, 
                 training_frame = data_train, 
                 validation_frame = data_valid,
                 hidden = c(32, 32, 32, 32, 32),
                 activation = "Tanh", # default is rectified linear, we can change to many other function, tanh for instance
                 # (tanh is tangent hyperbolic)
                 # after linearly combining the inputs and their weights, the function on them to create the neuron an be changed
                 seed = 123)
```

### 1.3.4 Experimenting with `epochs`

It can be set, how many times all training data points would be used to adjust the model in the course of optimization, it is called `epoch`. It is important to note that if early stopping criteria were set, the would be no guarantee to use all `epoch`. In this experience it is set to 100, early stopping is set to default and structure is from model `deep_small_model` with four hidden layers and 32 neurons on each layer. This structure will be used in all the coming models.

```{r}
more_epochs_model <- h2o.deeplearning(x = X, 
                 y = y, 
                 training_frame = data_train, 
                 validation_frame = data_valid,
                 hidden = c(32, 32, 32, 32, 32),
                 epochs = 200,
                 stopping_rounds = 0, # setting it to zero means there is no early stopping applied
                 seed = 123)
```

### 1.3.5 Experimenting with `batch`

It can be set that after how many training samples is the gradient update made, it is called `mini_batch_size`, default is one, not I am experimenting with ten.

```{r}
higher_batch_size_model <- h2o.deeplearning(x = X, 
                 y = y, 
                 training_frame = data_train, 
                 validation_frame = data_valid,
                 hidden = c(32, 32, 32, 32, 32),
                 mini_batch_size = 10, # increasing it will yield less precise results, but will be faster
                 seed = 123)
```

### 1.3.6 Experimenting with regularization

With attribute called `hidden_dropout_ratios` it can be set that with how large probability will neurons be left out of the model at a step, default is 0.5. There is a possibility to set different values to each layer. To use this kind of regularization a special activitation function is to set, called `WithDropout`, or in this case `RectifierWithDropout`.

```{r}
dropout_model <- h2o.deeplearning(x = X, 
                 y = y, 
                 training_frame = data_train, 
                 validation_frame = data_valid,
                 hidden = c(32, 32, 32, 32, 32),
                 activation = "RectifierWithDropout", # dropping out some nodes randomly by using an activation function
                 hidden_dropout_ratios = c(0.1, 0.1, 0.2, 0.2, 0.2), # a vector can also be used, length of vector equals to the number
                 # of hidden layers, each number tells what percent of nodes on that specific layer is to drop out randomly
                 seed = 123)
```

Another option to regularization is use attribute `input_dropout_ratio`, by using it some input features can be dropped randomly. In this experiment it is set to 0.4.

```{r}
input_dropout_model <- h2o.deeplearning(x = X, 
                 y = y, 
                 training_frame = data_train, 
                 validation_frame = data_valid,
                 hidden = c(32, 32, 32, 32, 32),
                 input_dropout_ratio = 0.4, # the above showed dropout technique can be applied to the input layer as well
                 seed = 123)
```

### 1.3.7 Experimenting with Lasso and Ridge regularization

Another option to do regularization is doing Lasso and Ridge by using attributes `l1` and `l2`, weight on $L1$ (lasso) and $L2$ (ridge) penalty terms.

```{r}
regularized_model <- h2o.deeplearning(x = X, 
                 y = y, 
                 training_frame = data_train, 
                 validation_frame = data_valid,
                 hidden = c(32, 32, 32, 32, 32),
                 # NN is a highly parameterized model, every weight is already a parameter
                 l1 = 0.001,
                 l2 = 0.001,
                 seed = 123)
```

### 1.3.8 Experimenting with early stopping options

Training constantly tracks validation frame performance. Early stopping is enabled by default but can be tuned when to stop. This, again, is to prevent overfitting. (If one does not supply a `validation_frame`, early stopping still works but based on metrics calculated from the training set = may not be as informative for out of sample performance.) There are early stopping options, such as `stopping_rounds`, `stoppnig_metric` and `stopping_tolerance`. In this experiment `stopping_rounds` is set to two, `stoppnig_metric` is `AUC` and `stopping_tolerance` is 0.01.

```{r}
early_stopping_model <- h2o.deeplearning(x = X, 
                 y = y, 
                 training_frame = data_train, 
                 validation_frame = data_valid, # at each update of the frame, the performance of the current state of the model is validated
                 # on the validation set, if the performance stops increasing, we stop the training
                 # with other words, the model we are building is not only validated at the end of training, but at each steps it is checked
                 # to see that it still worth to continue
                 hidden = c(32, 32, 32, 32, 32),
                 epochs = 100,
                 stopping_rounds = 2, # moving average is calculated, it takes the average of the last and previous results (default is five)
                 stopping_metric = "AUC", # metric of performance
                 stopping_tolerance = 0.01, # how much increase in AUC is need to continue the training process
                 seed = 123)
```

### 1.4 How does your best model compare to the benchmark model on the test set?

```{r}
# validation set performance

# checking performance of benchmark models and NNs on validation set
validation_performances <- list(
  "benchmark_rf" = h2o.performance(rf_model, data_valid)@metrics$AUC,
  "benchmark_gbm" = h2o.performance(gbm_model, data_valid)@metrics$AUC,
  "benchmark_dl" = h2o.performance(dl_model, data_valid)@metrics$AUC,
  "shallow_small_model" = h2o.performance(shallow_small_model, data_valid)@metrics$AUC,
  "shallow_large_model" = h2o.performance(shallow_large_model, data_valid)@metrics$AUC,
  "deep_small_model" = h2o.performance(deep_small_model, data_valid)@metrics$AUC,
  "deep_large_model" = h2o.performance(deep_large_model, data_valid)@metrics$AUC,
  "tanh_model" = h2o.performance(tanh_model, data_valid)@metrics$AUC,
  "more_epochs_model" = h2o.performance(more_epochs_model, data_valid)@metrics$AUC,
  "higher_batch_size_model" = h2o.performance(higher_batch_size_model, data_valid)@metrics$AUC,
  "dropout_model" = h2o.performance(dropout_model, data_valid)@metrics$AUC,
  "input_dropout_model" = h2o.performance(input_dropout_model, data_valid)@metrics$AUC,
  "regularized_model" = h2o.performance(regularized_model, data_valid)@metrics$AUC,
  "early_stopping_model" = h2o.performance(early_stopping_model, data_valid)@metrics$AUC
)

validation_performances
```

The `deep_large_model` had the best performance among the neural nets, but the `benchmark_gbm` outperformed it. So summarizing it, neural nets could outperform random forest in many cases, but gradient boosting machine model was far the best over neural nets.

### 1.5 Evaluate the model that performs best based on the validation set on the test set.

The best performing model was the `benchmark_gbm` benchmark model on the validation set with AUC of `0.7201964`.

```{r}
h2o.performance(gbm_model, data_valid)@metrics$AUC
h2o.performance(gbm_model, data_test)@metrics$AUC
```

Its performance on the test is close to the previous result, it was `AUC = 0.7187`.