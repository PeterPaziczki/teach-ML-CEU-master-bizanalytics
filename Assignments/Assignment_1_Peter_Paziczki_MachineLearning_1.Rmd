---
title: "Machine Learning 1 - Homework assignment 1"
author: "Peter Paziczki"
date: '2018 janu√°r 29'
output: html_document
---

<style>
body {
text-align: justify}
</style>

```{r setup, include=FALSE}
rm(list=ls())
knitr::opts_chunk$set(echo = TRUE)
```

#1. Model selection with a validation set

```{r}
## loading necessary packages
library(data.table)
library(ggplot2)
library(GGally)
library(lattice)
library(caret)

## loading the data
data <- fread("../data/king_county_house_prices/kc_house_data.csv")

## cleaning
data[, `:=`(floors = as.numeric(floors), zipcode = factor(zipcode))]
## creating log price
data[, log_price := log(price)]
## dropping unnecessary variables
data[, c("id", "date", "sqft_living15", "sqft_lot15", "price") := NULL]
```

### 1.1 Using createDataPartition, cut your data into three parts: 50% should be your training data, 25% each your validation and test sets (hint: cut data into two parts, then further cut one part into two).

```{r}
set.seed(1234)
training_ratio <- 0.5
train_indices_1 <- createDataPartition(y = data[["log_price"]],
                                     times = 1,
                                     p = training_ratio,
                                     list = FALSE)
data_train <- data[train_indices_1, ]
data_val_and_test <- data[-train_indices_1, ]
train_indices_2 <- createDataPartition(y = data_val_and_test[["log_price"]],
                                     times = 1,
                                     p = training_ratio,
                                     list = FALSE)
data_validation <- data_val_and_test[train_indices_2, ]
data_test <- data_val_and_test[-train_indices_2, ]
```

### 1.2 Train three models on the training data via caret, without cross validation (method = "none"):

+ a linear model lm with only using sqft_living as a predictor (a simple benchmark)
+ a linear model lm using all available features
+ a regression tree (rpart) with cp = 0.0001 (the tune grid should be a dataframe with one column cp and one row with value 0.0001)

```{r}
train_control <- trainControl(method = "none")
tune_grid <- data.frame("cp" = 0.0001)

simple_linear_fit <- train(log_price ~ sqft_living, 
                   data = data_train, 
                   method = "lm", 
                   trControl = train_control)

linear_fit <- train(log_price ~ ., 
                   data = data_train, 
                   method = "lm",
                   trControl = train_control)

rpart_fit <- train(log_price ~ ., 
                   data = data_train, 
                   method = "rpart", 
                   trControl = train_control,
                   tuneGrid = tune_grid)
```

### 1.3 Compare your models on the validation set and choose the one with the best performance (using RMSE). Use predict.train for prediction just like we used predict in class.

```{r}
RMSE <- function(x, true_x) sqrt(mean((x - true_x)^2))

simple_linear_rmse <- RMSE(predict.train(simple_linear_fit, data_validation), 
                          data_validation[["log_price"]])
#simple_linear_rmse_2 <- RMSE(predict.train(simple_linear_fit, newdata = data_validation, type = 'raw'), data_validation$log_price)
simple_linear_rmse

linear_rmse <- RMSE(predict.train(linear_fit, data_validation), 
                          data_validation[["log_price"]])
#linear_rmse_2 <- RMSE(predict.train(linear_fit, newdata = data_validation, type = 'raw'), data_validation$log_price)
linear_rmse

rpart_rmse <- RMSE(predict.train(rpart_fit, data_validation), 
                          data_validation[["log_price"]])
#rpart_rmse_2 <- RMSE(predict.train(rpart_fit, newdata = data_validation, type = 'raw'), data_validation$log_price)
rpart_rmse

performance_meause <- data.table("simple_linear_rmse" = simple_linear_rmse,
                     "linear_rmse" = linear_rmse,
                     "rpart_rmse" = rpart_rmse)
performance_meause
```

The linear fit with only one predictor gave the highest RMSE (```r simple_linear_rmse```), it is our benchmark. The linear modell and the regression tree gave better results using all the variables available, ```r linear_rmse``` and ```r rpart_rmse``` respectively. We can see that the linear model performed better compared to the regression tree.

### 1.4 Evaluate the final model on the test set. Why is it important to have this final set of observations set aside for evaluation? (Hint: think about what we used the validation set for.)

```{r}
final_performance_measure <- RMSE(predict.train(linear_fit, data_test), 
                          data_test[["log_price"]])
#final_performance_measure_2 <- RMSE(predict.train(linear_fit, newdata = data_test, type = 'raw'), data_test$log_price)
final_performance_measure
```

The three different data sets are used for different purposes. The training data set is used to fit the different models (two linear models and a regression tree in our case), while the validation data is used for model selection, the performance of different models are compared and the best is chosen. In our case the one that produced the lowest RMSE. Once the model is chosen, it is to be tested on a data set that was not used for training or model selection. On this test data the generalization error of the final model isto  be assessed.

### 1.5 Do you think it makes more sense to use this method rather than the one used in class? What can be advantages or disadvantages of one or the other?

When there is enough data, dividing it into parts is a good approach, because it is a good way of avoiding to overfit the model. In case of having scarce data unfortunately it is not an option to divide the data, because simply there wouldn't be enough data left to work with. To overcome that problem K-fold crossvalidation can be used It uses a part of data to fit the model and another to test. This way we can work with smaller data sets. In our case we have more than 21.000 observation, so I believe we can go with the method requested in the previous exercise.

## 2. Predicting developer salaries

```{r}
data <- fread("../data/stackoverflow2017/survey_results_public_selected.csv")

data <- data[!is.na(Salary) & Salary > 0]
data <- data[complete.cases(data)]
data <- data[, Gender := ifelse(Gender == "Male", "Male",
                              ifelse(Gender == "Female", "Female", "Other"))]
large_countries <- data[, .N, by = "Country"][N > 60][["Country"]]
data <- data[, Country := ifelse(Country %in% large_countries, Country, "Other")]
```

### 2.1 Describe what the data cleansing steps mean.

In this exercise the task is to predict developer salaries using the Stackoverflow Annual Developer Survey 2017.

The data is not always provided in a way or form that is requested to be able to work with that or to yield the best possible results. Some data transformation and / or conversation is always needed before model estimation. Data might be converted to a different type of format (categorical or dummy variables), missing values might be dropped or replaced. Numeric values might need to be normalized, binned, scaled (to be able to compare them), etc. Often the natural logarithm of variables is taken to have less skewed distribution that are more amenable for modeling purposes. 

In our case all the observation with missing salaries and zero salaries have been dropped. We have also dropped observations with NAs. In case of gender we have done some consolidation, there were missing values, transgenders, etc., no we have only three different genders female, male and others. We had observations from several countries, all the countries that were represented by less than 61 observation were categorized as "otehrs".

### 2.2 Using graphs, find at least two interesting features that can contribute to understanding developer salaries.

#### 2.2.1 Mean Salary by formal education
```{r}
## Mean salary by formal education
SalaryEdu <- data[, list(AvgSalary = mean(Salary)), by = FormalEducation][order(AvgSalary)]
ggplot(data = SalaryEdu, aes(x=FormalEducation, y=AvgSalary)) +
  geom_bar(stat = "identity") + theme(axis.text.x = element_text(angle = 90, hjust = 1))
```

#### 2.2.2 Mean Salary by country

```{r}
## Mean salary by country
SalaryCountry <- data[, list(AvgSalary = mean(Salary)), by = Country]
ggplot(data = SalaryCountry, aes(x=Country, y=AvgSalary)) + geom_bar(stat = "identity") +
  geom_bar(stat = "identity") + theme(axis.text.x = element_text(angle = 90, hjust = 1))
```

#### 2.2.3 Number of developers by gender

```{r}
## Number of people per Gender
ggplot(data, aes(Gender)) + geom_bar()
```

```{r}
## Distribution of salaries
ggplot(data, aes(Salary)) + geom_histogram()
```

```{r}
## Distribution of salaries by gender
ggplot(data, aes(Salary, fill = Gender)) + geom_density(alpha = 0.3) + theme_bw()
```

```{r}
## Boxplot of salaries by gender
ggplot(data, aes(Gender, Salary)) + geom_boxplot()
```

```{r}
## Distribution of salaries by formal education
ggplot(data, aes(Salary)) + geom_density()  + facet_wrap(~FormalEducation)
```

```{r}
## Number of people per company size
ggplot(data, aes(CompanySize)) + geom_bar()
```

```{r}
## Boxplot of salaries by FormalEducation
ggplot(data, aes(FormalEducation, Salary)) + geom_boxplot()
```

```{r}
#ggplot(data, aes(x = Salary, FormalEducation)) + geom_point(alpha = 0.1)
#ggplot(data, aes(x = Salary, YearsProgram)) + geom_point(alpha = 0.1)
#ggplot(data, aes(x = Salary, CompanySize)) + geom_point(alpha = 0.1)
#ggplot(data, aes(x = Salary, FormalEducation)) + geom_point(alpha = 0.1) + facet_wrap(~CompanySize)
#ggplot(data, aes(log(Salary))) + geom_histogram()
#ggplot(data, aes(Salary, fill = FormalEducation)) + geom_density(alpha = 0.3) + theme_bw()
#ggplot(data, aes(Salary)) + geom_density()  + facet_wrap(~CompanySize)
## Number of people per country
#ggplot(data, aes(Country)) + geom_bar()
## Number of people per ProgramHobby
#ggplot(data, aes(ProgramHobby)) + geom_bar()
```

### 2.3 Create a training and a test set assigning 70% to the training set and 30% as the test set.
### 2.4 Using caret train at least two predictive models to predict the logarithm of Salary (they can be of the same family but with different hyperparameters or they can be of different families like we used lm and rpart in the first exercise). Make sure NOT to include Salary as a predictor variable. Also, just before calling train, remember to use set.seed.

+ choose the best model based on cross-validation estimation on the training set
+ evaluate its performance on the test set
### 2.5 compare the true and predicted values of the test set on a graph. How do you evaluate the model fit based on this graph?